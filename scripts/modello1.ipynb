{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Using TensorFlow version', '1.8.0')\n",
      "('RNG seed: ', 420)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "tfk = tf.layers\n",
    "tfkl = tf.layers\n",
    "print(\"Using TensorFlow version\", tf.__version__)\n",
    "\n",
    "seed = 420\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "print(\"RNG seed: \",seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA LOADING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('/home/npasini1/Desktop/Recordings/nic_suture_10.csv')\n",
    "# dataframe.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BA' 'BA' 'BA' ... 'BA' 'BA' 'BA']\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding of Gestures\n",
    "from numpy import array, argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "gesture = dataframe['Gesture']\n",
    "values = array(gesture)\n",
    "print(values)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "\n",
    "encoded = to_categorical(integer_encoded)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "TB\n"
     ]
    }
   ],
   "source": [
    "print(integer_encoded[793])\n",
    "print(values[793])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.head of             0         1         2         3         4         5         6   \\\n",
       "0     0.099247  0.015124  0.445813  0.936466  0.140097  0.276233  0.493245   \n",
       "1     0.099247  0.015124  0.445813  0.936466  0.140097  0.276233  0.493245   \n",
       "2     0.099247  0.015124  0.445813  0.936466  0.140097  0.276233  0.493245   \n",
       "3     0.099247  0.015124  0.445813  0.936466  0.140097  0.276233  0.493245   \n",
       "4     0.099247  0.015124  0.445813  0.936466  0.140097  0.276233  0.493245   \n",
       "5     0.099247  0.015124  0.445813  0.936466  0.140097  0.276233  0.493245   \n",
       "6     0.099253  0.015128  0.445802  0.936452  0.140051  0.276307  0.493266   \n",
       "7     0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "8     0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "9     0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "10    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "11    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "12    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "13    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "14    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "15    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "16    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "17    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "18    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "19    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "20    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "21    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "22    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "23    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "24    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "25    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "26    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "27    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "28    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "29    0.099259  0.015132  0.445792  0.936437  0.140006  0.276380  0.493286   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6307  0.208801  0.810391  0.743886  0.937094  0.103427  0.516664  0.465029   \n",
       "6308  0.208781  0.810391  0.743906  0.937093  0.103428  0.516637  0.465062   \n",
       "6309  0.208759  0.810407  0.743926  0.937092  0.103427  0.516607  0.465094   \n",
       "6310  0.208720  0.810407  0.743967  0.937092  0.103426  0.516624  0.465066   \n",
       "6311  0.208740  0.810407  0.743946  0.937092  0.103427  0.516604  0.465096   \n",
       "6312  0.208761  0.810391  0.743927  0.937093  0.103429  0.516609  0.465096   \n",
       "6313  0.208761  0.810391  0.743927  0.937093  0.103429  0.516609  0.465096   \n",
       "6314  0.208820  0.810410  0.743968  0.937092  0.103427  0.516607  0.465094   \n",
       "6315  0.208791  0.810408  0.743998  0.937070  0.103391  0.516656  0.465153   \n",
       "6316  0.208771  0.810408  0.744019  0.937070  0.103391  0.516653  0.465155   \n",
       "6317  0.208813  0.810391  0.743979  0.937071  0.103393  0.516662  0.465153   \n",
       "6318  0.208813  0.810391  0.743979  0.937071  0.103393  0.516662  0.465153   \n",
       "6319  0.208831  0.810408  0.743957  0.937069  0.103392  0.516615  0.465212   \n",
       "6320  0.208791  0.810408  0.743998  0.937069  0.103392  0.516632  0.465184   \n",
       "6321  0.208771  0.810408  0.744019  0.937069  0.103393  0.516605  0.465217   \n",
       "6322  0.208827  0.810390  0.743977  0.937084  0.103420  0.516599  0.465310   \n",
       "6323  0.208829  0.810374  0.743978  0.937085  0.103421  0.516601  0.465312   \n",
       "6324  0.208847  0.810390  0.743956  0.937084  0.103420  0.516603  0.465308   \n",
       "6325  0.208807  0.810390  0.743997  0.937084  0.103420  0.516596  0.465312   \n",
       "6326  0.208788  0.810390  0.744018  0.937084  0.103420  0.516592  0.465314   \n",
       "6327  0.208827  0.810390  0.743977  0.937084  0.103420  0.516599  0.465310   \n",
       "6328  0.208799  0.810373  0.743957  0.937085  0.103421  0.516601  0.465312   \n",
       "6329  0.208767  0.810408  0.743959  0.937054  0.103368  0.516578  0.465182   \n",
       "6330  0.208765  0.810424  0.743957  0.937053  0.103367  0.516576  0.465180   \n",
       "6331  0.208765  0.810424  0.743957  0.937053  0.103367  0.516576  0.465180   \n",
       "6332  0.208745  0.810424  0.743978  0.937053  0.103367  0.516572  0.465182   \n",
       "6333  0.208767  0.810408  0.743959  0.937054  0.103368  0.516578  0.465182   \n",
       "6334  0.208787  0.810408  0.743938  0.937054  0.103367  0.516605  0.465149   \n",
       "6335  0.208765  0.810424  0.743957  0.937053  0.103367  0.516576  0.465180   \n",
       "6336  0.208765  0.810424  0.743957  0.937053  0.103367  0.516576  0.465180   \n",
       "\n",
       "            7         8         9   ...        23        24        25  \\\n",
       "0     0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "1     0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "2     0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "3     0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "4     0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "5     0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "6     0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "7     0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "8     0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "9     0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "10    0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "11    0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "12    0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "13    0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "14    0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "15    0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "16    0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "17    0.760139  0.583966  0.419558  ...  0.535697  0.686939  0.417284   \n",
       "18    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "19    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "20    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "21    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "22    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "23    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "24    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "25    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "26    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "27    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "28    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "29    0.760139  0.583967  0.419557  ...  0.535697  0.686939  0.417284   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "6307  0.432920  0.944330  0.742541  ...  0.535680  0.686880  0.417285   \n",
       "6308  0.432908  0.944330  0.742528  ...  0.535681  0.686884  0.417285   \n",
       "6309  0.432920  0.944330  0.742541  ...  0.535681  0.686886  0.417285   \n",
       "6310  0.432920  0.944330  0.742541  ...  0.535682  0.686888  0.417285   \n",
       "6311  0.432920  0.944330  0.742541  ...  0.535683  0.686891  0.417285   \n",
       "6312  0.432920  0.944330  0.742541  ...  0.535683  0.686892  0.417285   \n",
       "6313  0.432908  0.944330  0.742528  ...  0.535684  0.686894  0.417285   \n",
       "6314  0.432920  0.944330  0.742541  ...  0.535684  0.686896  0.417285   \n",
       "6315  0.432933  0.944330  0.742554  ...  0.535685  0.686897  0.417285   \n",
       "6316  0.432920  0.944330  0.742541  ...  0.535685  0.686899  0.417285   \n",
       "6317  0.432920  0.944330  0.742541  ...  0.535686  0.686900  0.417285   \n",
       "6318  0.432920  0.944330  0.742541  ...  0.535686  0.686901  0.417285   \n",
       "6319  0.432905  0.944326  0.742526  ...  0.535679  0.686992  0.417252   \n",
       "6320  0.432917  0.944326  0.742539  ...  0.535679  0.686992  0.417252   \n",
       "6321  0.432905  0.944326  0.742526  ...  0.535679  0.686992  0.417252   \n",
       "6322  0.432892  0.944326  0.742513  ...  0.535679  0.686992  0.417252   \n",
       "6323  0.432905  0.944326  0.742526  ...  0.535679  0.686992  0.417252   \n",
       "6324  0.432905  0.944326  0.742526  ...  0.535679  0.686992  0.417252   \n",
       "6325  0.432901  0.944322  0.742524  ...  0.535679  0.686992  0.417252   \n",
       "6326  0.432889  0.944322  0.742511  ...  0.535679  0.686992  0.417252   \n",
       "6327  0.432861  0.944318  0.742482  ...  0.535679  0.686992  0.417252   \n",
       "6328  0.432873  0.944318  0.742495  ...  0.535682  0.686985  0.417256   \n",
       "6329  0.432873  0.944318  0.742495  ...  0.535683  0.686981  0.417258   \n",
       "6330  0.432873  0.944318  0.742495  ...  0.535684  0.686976  0.417261   \n",
       "6331  0.432873  0.944318  0.742495  ...  0.535647  0.687088  0.417192   \n",
       "6332  0.432861  0.944318  0.742482  ...  0.535646  0.687091  0.417190   \n",
       "6333  0.432873  0.944318  0.742495  ...  0.535645  0.687094  0.417188   \n",
       "6334  0.432885  0.944318  0.742509  ...  0.535649  0.687081  0.417197   \n",
       "6335  0.432873  0.944318  0.742495  ...  0.535660  0.687048  0.417217   \n",
       "6336  0.432873  0.944318  0.742495  ...  0.535667  0.687028  0.417229   \n",
       "\n",
       "            26        27        28        29        30        31        32  \n",
       "0     0.740857  0.502366  0.834553  0.765404  0.471277  0.721779  0.840587  \n",
       "1     0.740857  0.502366  0.832253  0.765404  0.471277  0.724351  0.840587  \n",
       "2     0.740857  0.502366  0.836227  0.765404  0.471277  0.723065  0.840587  \n",
       "3     0.740857  0.502366  0.835808  0.765404  0.471277  0.724608  0.840587  \n",
       "4     0.740857  0.502366  0.835808  0.765404  0.471277  0.728208  0.840587  \n",
       "5     0.740857  0.502366  0.837482  0.765404  0.471277  0.727436  0.840587  \n",
       "6     0.740857  0.502366  0.834344  0.765404  0.471277  0.725379  0.840583  \n",
       "7     0.740857  0.502366  0.835181  0.765404  0.471277  0.722808  0.840579  \n",
       "8     0.740857  0.502366  0.831207  0.765404  0.471277  0.723322  0.840579  \n",
       "9     0.740857  0.502366  0.831834  0.765404  0.471277  0.723579  0.840579  \n",
       "10    0.740857  0.502366  0.836227  0.765404  0.471277  0.725894  0.840579  \n",
       "11    0.740857  0.502366  0.833089  0.765404  0.471277  0.724351  0.840579  \n",
       "12    0.740857  0.502366  0.833298  0.765404  0.471277  0.724865  0.840579  \n",
       "13    0.740857  0.502366  0.834135  0.765404  0.471277  0.726665  0.840579  \n",
       "14    0.740857  0.502366  0.836645  0.765404  0.471277  0.728465  0.840579  \n",
       "15    0.740857  0.502366  0.839364  0.765404  0.471277  0.726408  0.840579  \n",
       "16    0.740857  0.502366  0.834763  0.765404  0.471277  0.723836  0.840579  \n",
       "17    0.740857  0.502366  0.835181  0.765404  0.471277  0.724351  0.840579  \n",
       "18    0.740857  0.502366  0.836436  0.765404  0.471277  0.724865  0.840579  \n",
       "19    0.740857  0.502366  0.834344  0.765404  0.471277  0.726151  0.840579  \n",
       "20    0.740857  0.502366  0.836854  0.765404  0.471277  0.724094  0.840579  \n",
       "21    0.740857  0.502366  0.834763  0.765404  0.471277  0.728208  0.840579  \n",
       "22    0.740857  0.502366  0.838528  0.765404  0.471277  0.724608  0.840579  \n",
       "23    0.740857  0.502366  0.838737  0.765404  0.471277  0.724608  0.840579  \n",
       "24    0.740857  0.502366  0.835599  0.765404  0.471277  0.727693  0.840579  \n",
       "25    0.740857  0.502366  0.833508  0.765404  0.471277  0.725894  0.840579  \n",
       "26    0.740857  0.502366  0.836018  0.765404  0.471277  0.724608  0.840579  \n",
       "27    0.740857  0.502366  0.834344  0.765404  0.471277  0.722808  0.840579  \n",
       "28    0.740857  0.502366  0.836854  0.765404  0.471277  0.725379  0.840579  \n",
       "29    0.740857  0.502366  0.836227  0.765404  0.471277  0.726922  0.840579  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "6307  0.998823  0.502366  0.867810  0.990669  0.471277  0.757778  0.523883  \n",
       "6308  0.998901  0.502366  0.864254  0.990669  0.471277  0.757521  0.523883  \n",
       "6309  0.998980  0.502366  0.859862  0.990669  0.471277  0.757264  0.523902  \n",
       "6310  0.999058  0.502366  0.857770  0.990669  0.471277  0.757007  0.523922  \n",
       "6311  0.998980  0.502366  0.857770  0.990669  0.471277  0.760864  0.523912  \n",
       "6312  0.998980  0.502366  0.861326  0.990669  0.471277  0.758293  0.523903  \n",
       "6313  0.998980  0.502366  0.860489  0.990669  0.471277  0.756750  0.523893  \n",
       "6314  0.998980  0.502366  0.856306  0.990669  0.471277  0.758293  0.523866  \n",
       "6315  0.998980  0.502366  0.857561  0.990669  0.471277  0.758293  0.523891  \n",
       "6316  0.998980  0.502366  0.860699  0.990669  0.471277  0.757264  0.523891  \n",
       "6317  0.998980  0.502366  0.858607  0.990669  0.471277  0.755978  0.523872  \n",
       "6318  0.998980  0.502366  0.856725  0.990900  0.471277  0.749550  0.523872  \n",
       "6319  0.998823  0.502366  0.862581  0.990669  0.471255  0.749550  0.523846  \n",
       "6320  0.998744  0.502366  0.866346  0.990669  0.471255  0.756750  0.523877  \n",
       "6321  0.998980  0.502366  0.856934  0.990669  0.471255  0.758035  0.523877  \n",
       "6322  0.998901  0.502366  0.861954  0.990669  0.471255  0.759835  0.523840  \n",
       "6323  0.998901  0.502366  0.862372  0.990669  0.471255  0.759064  0.523852  \n",
       "6324  0.998901  0.502366  0.862790  0.990669  0.471255  0.758035  0.523840  \n",
       "6325  0.998901  0.502366  0.862790  0.990669  0.471255  0.755207  0.523856  \n",
       "6326  0.998901  0.502366  0.861117  0.990592  0.471255  0.757521  0.523856  \n",
       "6327  0.998901  0.502366  0.860071  0.990669  0.471256  0.758550  0.523812  \n",
       "6328  0.998901  0.502366  0.856934  0.990669  0.471258  0.758550  0.523842  \n",
       "6329  0.998901  0.502366  0.858398  0.990669  0.471259  0.754950  0.523854  \n",
       "6330  0.998901  0.502366  0.862581  0.990746  0.471261  0.756236  0.523852  \n",
       "6331  0.998901  0.502366  0.863418  0.990746  0.471216  0.749550  0.523852  \n",
       "6332  0.998901  0.502366  0.861744  0.990746  0.471214  0.752121  0.523852  \n",
       "6333  0.998901  0.502366  0.857980  0.990746  0.471213  0.753150  0.523854  \n",
       "6334  0.998980  0.502366  0.857980  0.990746  0.471219  0.752636  0.523854  \n",
       "6335  0.998901  0.502366  0.859234  0.990746  0.471232  0.753150  0.523852  \n",
       "6336  0.998901  0.502366  0.858398  0.990746  0.471240  0.749550  0.523852  \n",
       "\n",
       "[6337 rows x 33 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data normalization\n",
    "\n",
    "labels = encoded\n",
    "df = dataframe.drop(['Gesture','Timestamps'], axis=1)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_scaled = min_max_scaler.fit_transform(df)\n",
    "df_scaled = pd.DataFrame(df_scaled)\n",
    "df_scaled.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6337, 33)\n"
     ]
    }
   ],
   "source": [
    "print(df_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6332, 5, 33)\n"
     ]
    }
   ],
   "source": [
    "samples = df_scaled.shape[0]\n",
    "n_features = df_scaled.shape[1]\n",
    "timestep = 5\n",
    "stride = 1\n",
    "\n",
    "dataset = np.empty((0,timestep,n_features))\n",
    "# np_scaled = np.array(df_scaled)\n",
    "# print(dataset.shape)\n",
    "\n",
    "# CON PREPROCESSING DEI DATI: MINMAX SCALER\n",
    "\n",
    "# for idx in range(samples-timestep):\n",
    "#     dataset = np.append(dataset, np.expand_dims(df_scaled[idx:idx+timestep], axis=0),axis=0)\n",
    "#     # print(idx)\n",
    "\n",
    "# SENZA PREPROCESSING DEI DATI\n",
    "\n",
    "for idx in range(samples-timestep):\n",
    "    dataset = np.append(dataset, np.expand_dims(df[idx:idx+timestep], axis=0),axis=0)\n",
    "\n",
    "print(dataset.shape)\n",
    "dataset_labels = labels[timestep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6332, 4)\n"
     ]
    }
   ],
   "source": [
    "dataset_labels = labels[timestep:]\n",
    "print(dataset_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train shape: ', (5698, 5, 33))\n",
      "('Test shape: ', (634, 5, 33))\n",
      "('Input shape: ', (5, 33))\n",
      "('Output shape: ', (4,))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, dataset_labels, test_size=0.10, shuffle = True)\n",
    "print(\"Train shape: \", X_train.shape)\n",
    "print(\"Test shape: \", X_test.shape)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "output_shape = y_train.shape[1:]\n",
    "print(\"Input shape: \", input_shape)\n",
    "print(\"Output shape: \", output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/npasini1/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1044: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/npasini1/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/npasini1/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 5, 33)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 128)            82944     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 5, 128)            512       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 320)               1280      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                20544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 155,460.0\n",
      "Trainable params: 154,308.0\n",
      "Non-trainable params: 1,152.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as tfk\n",
    "import keras.layers as tfkl\n",
    "import scipy.io\n",
    "import os, glob, sys, pickle, time, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras import layers #Dense, LSTM, RepeatVector, TimeDistributed, Dropout, Masking, BatchNormalization, Flatten, Input, Conv2D, MaxPooling1D, Conv1D, Reshape, GRU\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, CSVLogger\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "from datetime import datetime\n",
    "\n",
    "def buildModelv2(timesteps, n_features):\n",
    "    \"\"\"\n",
    "    An lstm-encoder model followed by dense layers, similar performance to just lstm\n",
    "    \"\"\"\n",
    "    # lstm_hidden1 = 128 #*4\n",
    "    # lstm_hidden2 = 64\n",
    "    # dense_hidden1 = 64\n",
    "    # output_layer = 15\n",
    "\n",
    "    model_input = layers.Input(shape=input_shape)\n",
    "    lstm_output = layers.LSTM(128, input_shape=input_shape, kernel_regularizer=keras.regularizers.l1(l=0.001), return_sequences=True)(model_input) #previously 96\n",
    "    dropout_output = layers.Dropout(rate=0.2)(lstm_output) #previously 0.4\n",
    "    batch_norm1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(dropout_output)\n",
    "    lstm_output2 = layers.LSTM(64, input_shape=input_shape, kernel_regularizer=keras.regularizers.l1(l=0.001), return_sequences=False)(batch_norm1)\n",
    "    dropout_output = layers.Dropout(rate = 0.3)(lstm_output2)\n",
    "    batch_norm2 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(dropout_output)\n",
    "    repeat_vector = layers.RepeatVector(timestep)(batch_norm2)\n",
    "\n",
    "    flatten_output = layers.Flatten()(repeat_vector)\n",
    "    batch_norm2 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(flatten_output)\n",
    "    dropout_output = layers.Dropout(rate = 0.2)(batch_norm2)\n",
    "    dense_output1 = layers.Dense(64, activation='relu')(dropout_output)\n",
    "    batch_norm3 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(dense_output1)\n",
    "    dense_output2 = layers.Dense(4, activation='softmax')(batch_norm3)\n",
    "    lstm_classifier = Model(inputs=model_input, outputs=dense_output2)\n",
    "    \n",
    "    #Compile the model\n",
    "    lstm_classifier.compile(loss=tfk.losses.categorical_crossentropy, optimizer=tfk.optimizers.Adam(1e-3), metrics=[keras.metrics.categorical_accuracy])\n",
    "\n",
    "    return lstm_classifier\n",
    "\n",
    "def buildModelv1(timesteps, n_features):\n",
    "    \"\"\"\n",
    "    Simple model which yields a high accuracy of 90% on training data but sucks on the validation data\n",
    "    \"\"\"\n",
    "    model_input = layers.Input(shape=input_shape)\n",
    "    lstm_output = layers.LSTM(128, input_shape=input_shape, kernel_regularizer=keras.regularizers.l1(l=0.001), return_sequences=True)(model_input)\n",
    "    dropout_output = layers.Dropout(rate=0.2)(lstm_output)\n",
    "    flatten_output = layers.Flatten()(dropout_output)\n",
    "    dense_output1 = layers.Dense(64, activation='relu')(flatten_output)\n",
    "    dropout_output2 = layers.Dropout(rate=0.2)(dense_output1)\n",
    "    dense_output2 = layers.Dense(4, activation='softmax')(dropout_output2)\n",
    "    lstm_classifier = Model(model_input, dense_output2)\n",
    "    #lstm_classifier.summary()\n",
    "    lstm_classifier.compile(loss=tfk.losses.categorical_crossentropy, optimizer=tfk.optimizers.Adam(1e-3))\n",
    "\n",
    "    return lstm_classifier\n",
    "\n",
    "model = buildModelv2(timestep, n_features)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = keras.callbacks.ModelCheckpoint('prova.h5',\n",
    "                                        monitor='val_loss',\n",
    "                                        mode = 'min',\n",
    "                                        save_best_only=True,\n",
    "                                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5128 samples, validate on 570 samples\n",
      "Epoch 1/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.5749 - categorical_accuracy: 0.9502Epoch 00000: val_loss improved from 0.70185 to 0.55646, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.5744 - categorical_accuracy: 0.9503 - val_loss: 0.5565 - val_categorical_accuracy: 0.9421\n",
      "Epoch 2/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.4960 - categorical_accuracy: 0.9551Epoch 00001: val_loss improved from 0.55646 to 0.45494, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.4949 - categorical_accuracy: 0.9555 - val_loss: 0.4549 - val_categorical_accuracy: 0.9526\n",
      "Epoch 3/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.4336 - categorical_accuracy: 0.9589Epoch 00002: val_loss improved from 0.45494 to 0.42640, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.4336 - categorical_accuracy: 0.9590 - val_loss: 0.4264 - val_categorical_accuracy: 0.9474\n",
      "Epoch 4/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.3946 - categorical_accuracy: 0.9616Epoch 00003: val_loss improved from 0.42640 to 0.40511, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.3950 - categorical_accuracy: 0.9612 - val_loss: 0.4051 - val_categorical_accuracy: 0.9667\n",
      "Epoch 5/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.3437 - categorical_accuracy: 0.9699Epoch 00004: val_loss improved from 0.40511 to 0.36129, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.3430 - categorical_accuracy: 0.9700 - val_loss: 0.3613 - val_categorical_accuracy: 0.9614\n",
      "Epoch 6/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.3319 - categorical_accuracy: 0.9654Epoch 00005: val_loss improved from 0.36129 to 0.34185, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.3321 - categorical_accuracy: 0.9653 - val_loss: 0.3418 - val_categorical_accuracy: 0.9474\n",
      "Epoch 7/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.2991 - categorical_accuracy: 0.9699Epoch 00006: val_loss improved from 0.34185 to 0.29811, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.3002 - categorical_accuracy: 0.9696 - val_loss: 0.2981 - val_categorical_accuracy: 0.9632\n",
      "Epoch 8/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.3005 - categorical_accuracy: 0.9672Epoch 00007: val_loss did not improve\n",
      "5128/5128 [==============================] - 1s - loss: 0.3012 - categorical_accuracy: 0.9672 - val_loss: 0.3257 - val_categorical_accuracy: 0.9509\n",
      "Epoch 9/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.2652 - categorical_accuracy: 0.9761Epoch 00008: val_loss improved from 0.29811 to 0.24646, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.2650 - categorical_accuracy: 0.9762 - val_loss: 0.2465 - val_categorical_accuracy: 0.9684\n",
      "Epoch 10/1000\n",
      "4992/5128 [============================>.] - ETA: 0s - loss: 0.2415 - categorical_accuracy: 0.9744Epoch 00009: val_loss improved from 0.24646 to 0.24411, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.2409 - categorical_accuracy: 0.9746 - val_loss: 0.2441 - val_categorical_accuracy: 0.9754\n",
      "Epoch 11/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.2500 - categorical_accuracy: 0.9707Epoch 00010: val_loss did not improve\n",
      "5128/5128 [==============================] - 1s - loss: 0.2506 - categorical_accuracy: 0.9704 - val_loss: 0.2486 - val_categorical_accuracy: 0.9772\n",
      "Epoch 12/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.2401 - categorical_accuracy: 0.9771Epoch 00011: val_loss improved from 0.24411 to 0.20369, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.2392 - categorical_accuracy: 0.9774 - val_loss: 0.2037 - val_categorical_accuracy: 0.9807\n",
      "Epoch 13/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.2123 - categorical_accuracy: 0.9782Epoch 00012: val_loss improved from 0.20369 to 0.19792, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.2121 - categorical_accuracy: 0.9784 - val_loss: 0.1979 - val_categorical_accuracy: 0.9807\n",
      "Epoch 14/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.2155 - categorical_accuracy: 0.9771Epoch 00013: val_loss did not improve\n",
      "5128/5128 [==============================] - 1s - loss: 0.2153 - categorical_accuracy: 0.9770 - val_loss: 0.2004 - val_categorical_accuracy: 0.9789\n",
      "Epoch 15/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.2005 - categorical_accuracy: 0.9816Epoch 00014: val_loss did not improve\n",
      "5128/5128 [==============================] - 1s - loss: 0.2004 - categorical_accuracy: 0.9817 - val_loss: 0.2049 - val_categorical_accuracy: 0.9754\n",
      "Epoch 16/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.2160 - categorical_accuracy: 0.9749Epoch 00015: val_loss did not improve\n",
      "5128/5128 [==============================] - 1s - loss: 0.2184 - categorical_accuracy: 0.9748 - val_loss: 0.2990 - val_categorical_accuracy: 0.9561\n",
      "Epoch 17/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.1736 - categorical_accuracy: 0.9838Epoch 00016: val_loss improved from 0.19792 to 0.16856, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.1736 - categorical_accuracy: 0.9836 - val_loss: 0.1686 - val_categorical_accuracy: 0.9825\n",
      "Epoch 18/1000\n",
      "5120/5128 [============================>.] - ETA: 0s - loss: 0.1572 - categorical_accuracy: 0.9863Epoch 00017: val_loss improved from 0.16856 to 0.15554, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.1572 - categorical_accuracy: 0.9863 - val_loss: 0.1555 - val_categorical_accuracy: 0.9842\n",
      "Epoch 19/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.1460 - categorical_accuracy: 0.9883Epoch 00018: val_loss improved from 0.15554 to 0.14860, saving model to prova.h5\n",
      "5128/5128 [==============================] - 1s - loss: 0.1463 - categorical_accuracy: 0.9883 - val_loss: 0.1486 - val_categorical_accuracy: 0.9807\n",
      "Epoch 20/1000\n",
      "5056/5128 [============================>.] - ETA: 0s - loss: 0.1458 - categorical_accuracy: 0.9877Epoch 00019: val_loss did not improve\n",
      "5128/5128 [==============================] - 1s - loss: 0.1453 - categorical_accuracy: 0.9879 - val_loss: 0.1580 - val_categorical_accuracy: 0.9807\n",
      "Epoch 21/1000\n",
      "2368/5128 [============>.................] - ETA: 0s - loss: 0.1361 - categorical_accuracy: 0.9890"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-297b62c30369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtfk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtfk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     ]\n\u001b[1;32m     16\u001b[0m ).history\n",
      "\u001b[0;32m/home/npasini1/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/npasini1/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/npasini1/.local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/npasini1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/npasini1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/npasini1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/npasini1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/npasini1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/npasini1/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 1000,\n",
    "    validation_split=.10,\n",
    "    #validation_steps=10,\n",
    "    shuffle=False,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=200),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=2, factor=0.5, min_lr=1e-5),\n",
    "        mc\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('/home/npasini1/nicolo_ws/src/pyrecorder/src/bestmodel_v2_senzapreprocessing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(X_test)\n",
    "print(X_test.shape)\n",
    "prediction = np.argmax(a, axis=1)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 2.7.17 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
