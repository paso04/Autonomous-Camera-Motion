{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Using TensorFlow version', '1.8.0')\n",
      "('RNG seed: ', 420)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import array, argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "tfk = tf.layers\n",
    "tfkl = tf.layers\n",
    "print(\"Using TensorFlow version\", tf.__version__)\n",
    "\n",
    "seed = 420\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "print(\"RNG seed: \",seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathName = '/home/npasini1/Desktop/Recordings/new'\n",
    "numFiles = []\n",
    "\n",
    "timestep = 30\n",
    "stride = 1\n",
    "n_features = 33\n",
    "n_gestures = 4\n",
    "dataset_tot = np.empty((0,timestep,n_features))\n",
    "dataset_gesture_array_tot = np.empty((0))\n",
    "\n",
    "fileNames = os.listdir(pathName)\n",
    "for fileNames in fileNames:\n",
    "    if fileNames.endswith(\".csv\"):\n",
    "        numFiles.append(fileNames)\n",
    "        path = os.path.join(pathName, fileNames)\n",
    "        print('Now reading file: ', path)\n",
    "        dataframe = pd.read_csv(path)\n",
    "\n",
    "        #encoding\n",
    "        gesture = dataframe['Predicted Gesture']\n",
    "        gesture_array = array(gesture)\n",
    "\n",
    "        df = dataframe.drop(['Predicted Gesture','Timestamps'], axis=1)\n",
    "        df = pd.DataFrame(df)\n",
    "\n",
    "        samples = df.shape[0]\n",
    "        # n_features = df.shape[1]\n",
    "\n",
    "        dataset = np.empty((0,timestep,n_features))\n",
    "        df_copy = df\n",
    "\n",
    "        for idx in range(samples-timestep):\n",
    "            for j in range(timestep):\n",
    "                df_copy.iloc[idx+j,[0,1,2,7,8,9]] = df.iloc[idx+j, [0,1,2,7,8,9]] - df.iloc[idx, [0,1,2,7,8,9]]\n",
    "            dataset = np.append(dataset, np.expand_dims(df_copy[idx:idx+timestep], axis=0),axis=0)\n",
    "            j=0\n",
    "        \n",
    "        dataset_gesture_array = gesture_array[timestep:]\n",
    "        dataset_tot = np.append(dataset_tot,dataset, axis=0)\n",
    "        dataset_gesture_array_tot = np.append(dataset_gesture_array_tot, dataset_gesture_array, axis=0)\n",
    "        print(dataset.shape)\n",
    "        print(dataset_tot.shape)\n",
    "        print(dataset_gesture_array.shape)\n",
    "        print(dataset_gesture_array_tot.shape)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(dataset_gesture_array_tot)\n",
    "print(integer_encoded)\n",
    "\n",
    "encoded = to_categorical(integer_encoded)\n",
    "print(encoded)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 30\n",
    "stride = 1\n",
    "n_features = 33\n",
    "n_gestures = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"/home/npasini1/Desktop/Datasets/dataset_Nicolo_1to16.npy\", dataset_tot)\n",
    "# np.save(\"/home/npasini1/Desktop/Datasets/labels_Nicolo_1to16.npy\", encoded)\n",
    "dataset_tot = np.load(\"/home/npasini1/Desktop/Datasets/dataset_Nicolo_1to16.npy\")\n",
    "encoded = np.load(\"/home/npasini1/Desktop/Datasets/labels_Nicolo_1to16.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train shape: ', (37220, 30, 33))\n",
      "('Test shape: ', (4136, 30, 33))\n",
      "('Input shape: ', (30, 33))\n",
      "('Output shape: ', (4,))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset_tot, encoded, test_size=0.10, shuffle = True)\n",
    "print(\"Train shape: \", X_train.shape)\n",
    "print(\"Test shape: \", X_test.shape)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "output_shape = y_train.shape[1:]\n",
    "print(\"Input shape: \", input_shape)\n",
    "print(\"Output shape: \", output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = pd.read_csv('/home/npasini1/Desktop/Recordings/nic_suture_10.csv')\n",
    "dataframe = pd.read_csv('/home/npasini1/Desktop/Recordings/new/Nicolo_1.csv')\n",
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of Gestures\n",
    "\n",
    "gesture = dataframe['Predicted Gesture']\n",
    "values = array(gesture)\n",
    "print(values)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "\n",
    "encoded = to_categorical(integer_encoded)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(integer_encoded[793])\n",
    "print(values[793])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "\n",
    "labels = encoded\n",
    "df = dataframe.drop(['Predicted Gesture','Timestamps'], axis=1)\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# df_scaled = min_max_scaler.fit_transform(df)\n",
    "# df_scaled = pd.DataFrame(df_scaled)\n",
    "# df_scaled.head\n",
    "\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df.shape[0]\n",
    "n_features = df.shape[1]\n",
    "timestep = 30\n",
    "stride = 1\n",
    "\n",
    "dataset = np.empty((0,timestep,n_features))\n",
    "df_copy = df\n",
    "# np_scaled = np.array(df_scaled)\n",
    "# print(dataset.shape)\n",
    "\n",
    "# CON PREPROCESSING DEI DATI: MINMAX SCALER\n",
    "\n",
    "# for idx in range(samples-timestep):\n",
    "#     dataset = np.append(dataset, np.expand_dims(df_scaled[idx:idx+timestep], axis=0),axis=0)\n",
    "#     # print(idx)\n",
    "\n",
    "# SENZA PREPROCESSING DEI DATI\n",
    "\n",
    "\n",
    "for idx in range(samples-timestep):\n",
    "    for j in range(timestep):\n",
    "        df_copy.iloc[idx+j,[0,1,2,7,8,9]] = df.iloc[idx+j, [0,1,2,7,8,9]] - df.iloc[idx, [0,1,2,7,8,9]]\n",
    "    dataset = np.append(dataset, np.expand_dims(df_copy[idx:idx+timestep], axis=0),axis=0)\n",
    "    j=0\n",
    "\n",
    "print(dataset.shape)\n",
    "dataset_labels = labels[timestep:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels = labels[timestep:]\n",
    "print(dataset_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, dataset_labels, test_size=0.10, shuffle = True)\n",
    "print(\"Train shape: \", X_train.shape)\n",
    "print(\"Test shape: \", X_test.shape)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "output_shape = y_train.shape[1:]\n",
    "print(\"Input shape: \", input_shape)\n",
    "print(\"Output shape: \", output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 30, 33)            0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 30, 128)           82944     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 30, 128)           512       \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 30, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 1920)              7680      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                122944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 264,260.0\n",
      "Trainable params: 259,908.0\n",
      "Non-trainable params: 4,352.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as tfk\n",
    "import keras.layers as tfkl\n",
    "import scipy.io\n",
    "import os, glob, sys, pickle, time, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras import layers #Dense, LSTM, RepeatVector, TimeDistributed, Dropout, Masking, BatchNormalization, Flatten, Input, Conv2D, MaxPooling1D, Conv1D, Reshape, GRU\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, CSVLogger\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "from datetime import datetime\n",
    "\n",
    "n_gestures = 4\n",
    "\n",
    "def buildModelv2(timesteps, n_features):\n",
    "    \"\"\"\n",
    "    An lstm-encoder model followed by dense layers, similar performance to just lstm\n",
    "    \"\"\"\n",
    "    # lstm_hidden1 = 128 #*4\n",
    "    # lstm_hidden2 = 64\n",
    "    # dense_hidden1 = 64\n",
    "    # output_layer = 15\n",
    "\n",
    "    model_input = layers.Input(shape=input_shape)\n",
    "    lstm_output = layers.LSTM(128, input_shape=input_shape, kernel_regularizer=keras.regularizers.l1(l=0.001), return_sequences=True)(model_input) #previously 96\n",
    "    dropout_output = layers.Dropout(rate=0.2)(lstm_output) #previously 0.4\n",
    "    batch_norm1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(dropout_output)\n",
    "    lstm_output2 = layers.LSTM(64, input_shape=input_shape, kernel_regularizer=keras.regularizers.l1(l=0.001), return_sequences=False)(batch_norm1)\n",
    "    dropout_output = layers.Dropout(rate = 0.3)(lstm_output2)\n",
    "    batch_norm2 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(dropout_output)\n",
    "    repeat_vector = layers.RepeatVector(timestep)(batch_norm2)\n",
    "\n",
    "    flatten_output = layers.Flatten()(repeat_vector)\n",
    "    batch_norm2 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(flatten_output)\n",
    "    dropout_output = layers.Dropout(rate = 0.2)(batch_norm2)\n",
    "    dense_output1 = layers.Dense(64, activation='relu')(dropout_output)\n",
    "    batch_norm3 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True)(dense_output1)\n",
    "    dense_output2 = layers.Dense(4, activation='softmax')(batch_norm3)\n",
    "    lstm_classifier = Model(inputs=model_input, outputs=dense_output2)\n",
    "    \n",
    "    #Compile the model\n",
    "    lstm_classifier.compile(loss=tfk.losses.categorical_crossentropy, optimizer=tfk.optimizers.Adam(0.5e-2), metrics=[keras.metrics.categorical_accuracy])\n",
    "\n",
    "    return lstm_classifier\n",
    "\n",
    "def buildModelv1(timesteps, n_features):\n",
    "    \"\"\"\n",
    "    Simple model which yields a high accuracy of 90% on training data but sucks on the validation data\n",
    "    \"\"\"\n",
    "    model_input = layers.Input(shape=input_shape)\n",
    "    lstm_output = layers.LSTM(128, input_shape=input_shape, kernel_regularizer=keras.regularizers.l1(l=0.001), return_sequences=True)(model_input)\n",
    "    dropout_output = layers.Dropout(rate=0.2)(lstm_output)\n",
    "    flatten_output = layers.Flatten()(dropout_output)\n",
    "    dense_output1 = layers.Dense(64, activation='relu')(flatten_output)\n",
    "    dropout_output2 = layers.Dropout(rate=0.2)(dense_output1)\n",
    "    dense_output2 = layers.Dense(4, activation='softmax')(dropout_output2)\n",
    "    lstm_classifier = Model(model_input, dense_output2)\n",
    "    #lstm_classifier.summary()\n",
    "    lstm_classifier.compile(loss=tfk.losses.categorical_crossentropy, optimizer=tfk.optimizers.Adam(1e-3))\n",
    "\n",
    "    return lstm_classifier\n",
    "\n",
    "model = buildModelv2(timestep, n_features)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = keras.callbacks.ModelCheckpoint('/home/npasini1/Desktop/model_checkpoints/pyrecorder_Nicolo_1to16_allepochs_allsamples.h5',\n",
    "                                        monitor='val_loss',\n",
    "                                        mode = 'min',\n",
    "                                        save_best_only=True,\n",
    "                                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37220 samples, validate on 4136 samples\n",
      "Epoch 1/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 2.0007 - categorical_accuracy: 0.7755Epoch 00000: val_loss improved from inf to 0.79387, saving model to /home/npasini1/Desktop/model_checkpoints/pyrecorder_Nicolo_1to16_allepochs_allsamples.h5\n",
      "37220/37220 [==============================] - 53s - loss: 1.9983 - categorical_accuracy: 0.7756 - val_loss: 0.7939 - val_categorical_accuracy: 0.9594\n",
      "Epoch 2/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9537 - categorical_accuracy: 0.8345Epoch 00001: val_loss improved from 0.79387 to 0.63435, saving model to /home/npasini1/Desktop/model_checkpoints/pyrecorder_Nicolo_1to16_allepochs_allsamples.h5\n",
      "37220/37220 [==============================] - 46s - loss: 0.9538 - categorical_accuracy: 0.8344 - val_loss: 0.6344 - val_categorical_accuracy: 0.9657\n",
      "Epoch 3/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8865 - categorical_accuracy: 0.8617Epoch 00002: val_loss improved from 0.63435 to 0.46081, saving model to /home/npasini1/Desktop/model_checkpoints/pyrecorder_Nicolo_1to16_allepochs_allsamples.h5\n",
      "37220/37220 [==============================] - 43s - loss: 0.8863 - categorical_accuracy: 0.8616 - val_loss: 0.4608 - val_categorical_accuracy: 0.9659\n",
      "Epoch 4/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9652 - categorical_accuracy: 0.8591Epoch 00003: val_loss did not improve\n",
      "37220/37220 [==============================] - 44s - loss: 0.9647 - categorical_accuracy: 0.8593 - val_loss: 0.5054 - val_categorical_accuracy: 0.9628\n",
      "Epoch 5/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8924 - categorical_accuracy: 0.8629Epoch 00004: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.8921 - categorical_accuracy: 0.8631 - val_loss: 0.6656 - val_categorical_accuracy: 0.9487\n",
      "Epoch 6/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8530 - categorical_accuracy: 0.8694Epoch 00005: val_loss did not improve\n",
      "37220/37220 [==============================] - 45s - loss: 0.8525 - categorical_accuracy: 0.8695 - val_loss: 0.5528 - val_categorical_accuracy: 0.9400\n",
      "Epoch 7/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9732 - categorical_accuracy: 0.8609Epoch 00006: val_loss did not improve\n",
      "37220/37220 [==============================] - 50s - loss: 0.9735 - categorical_accuracy: 0.8610 - val_loss: 0.8543 - val_categorical_accuracy: 0.9057\n",
      "Epoch 8/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8687 - categorical_accuracy: 0.8737Epoch 00007: val_loss did not improve\n",
      "37220/37220 [==============================] - 55s - loss: 0.8678 - categorical_accuracy: 0.8740 - val_loss: 0.5013 - val_categorical_accuracy: 0.9577\n",
      "Epoch 9/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6794 - categorical_accuracy: 0.8817Epoch 00008: val_loss improved from 0.46081 to 0.40943, saving model to /home/npasini1/Desktop/model_checkpoints/pyrecorder_Nicolo_1to16_allepochs_allsamples.h5\n",
      "37220/37220 [==============================] - 46s - loss: 0.6787 - categorical_accuracy: 0.8819 - val_loss: 0.4094 - val_categorical_accuracy: 0.9565\n",
      "Epoch 10/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.0954 - categorical_accuracy: 0.8235Epoch 00009: val_loss did not improve\n",
      "37220/37220 [==============================] - 52s - loss: 1.0948 - categorical_accuracy: 0.8236 - val_loss: 0.8545 - val_categorical_accuracy: 0.8849\n",
      "Epoch 11/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8721 - categorical_accuracy: 0.8436Epoch 00010: val_loss did not improve\n",
      "37220/37220 [==============================] - 58s - loss: 0.8713 - categorical_accuracy: 0.8438 - val_loss: 0.4657 - val_categorical_accuracy: 0.9654\n",
      "Epoch 12/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9086 - categorical_accuracy: 0.8598Epoch 00011: val_loss did not improve\n",
      "37220/37220 [==============================] - 57s - loss: 0.9085 - categorical_accuracy: 0.8599 - val_loss: 0.7300 - val_categorical_accuracy: 0.9529\n",
      "Epoch 13/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8111 - categorical_accuracy: 0.8632Epoch 00012: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.8105 - categorical_accuracy: 0.8633 - val_loss: 0.4443 - val_categorical_accuracy: 0.9625\n",
      "Epoch 14/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7482 - categorical_accuracy: 0.8654Epoch 00013: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.7477 - categorical_accuracy: 0.8654 - val_loss: 0.4230 - val_categorical_accuracy: 0.9637\n",
      "Epoch 15/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6450 - categorical_accuracy: 0.8659Epoch 00014: val_loss improved from 0.40943 to 0.34019, saving model to /home/npasini1/Desktop/model_checkpoints/pyrecorder_Nicolo_1to16_allepochs_allsamples.h5\n",
      "37220/37220 [==============================] - 41s - loss: 0.6449 - categorical_accuracy: 0.8658 - val_loss: 0.3402 - val_categorical_accuracy: 0.9620\n",
      "Epoch 16/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6410 - categorical_accuracy: 0.8752Epoch 00015: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6410 - categorical_accuracy: 0.8751 - val_loss: 0.4299 - val_categorical_accuracy: 0.9599\n",
      "Epoch 17/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6249 - categorical_accuracy: 0.8738Epoch 00016: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6260 - categorical_accuracy: 0.8733 - val_loss: 0.4155 - val_categorical_accuracy: 0.9492\n",
      "Epoch 18/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6280 - categorical_accuracy: 0.8736Epoch 00017: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6278 - categorical_accuracy: 0.8737 - val_loss: 0.4314 - val_categorical_accuracy: 0.9666\n",
      "Epoch 19/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6520 - categorical_accuracy: 0.8778Epoch 00018: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6517 - categorical_accuracy: 0.8780 - val_loss: 0.4545 - val_categorical_accuracy: 0.9635\n",
      "Epoch 20/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7186 - categorical_accuracy: 0.8750Epoch 00019: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.7180 - categorical_accuracy: 0.8753 - val_loss: 0.3781 - val_categorical_accuracy: 0.9683\n",
      "Epoch 21/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6265 - categorical_accuracy: 0.8773Epoch 00020: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6269 - categorical_accuracy: 0.8772 - val_loss: 0.4978 - val_categorical_accuracy: 0.9630\n",
      "Epoch 22/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6315 - categorical_accuracy: 0.8859Epoch 00021: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6309 - categorical_accuracy: 0.8861 - val_loss: 0.3646 - val_categorical_accuracy: 0.9541\n",
      "Epoch 23/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9881 - categorical_accuracy: 0.8694Epoch 00022: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.9890 - categorical_accuracy: 0.8691 - val_loss: 1.0373 - val_categorical_accuracy: 0.8919\n",
      "Epoch 24/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9032 - categorical_accuracy: 0.8727Epoch 00023: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.9024 - categorical_accuracy: 0.8729 - val_loss: 0.5769 - val_categorical_accuracy: 0.9662\n",
      "Epoch 25/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7527 - categorical_accuracy: 0.8818Epoch 00024: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.7523 - categorical_accuracy: 0.8817 - val_loss: 0.4747 - val_categorical_accuracy: 0.9572\n",
      "Epoch 26/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6681 - categorical_accuracy: 0.8913Epoch 00025: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6685 - categorical_accuracy: 0.8915 - val_loss: 0.8272 - val_categorical_accuracy: 0.9500\n",
      "Epoch 27/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7562 - categorical_accuracy: 0.8866Epoch 00026: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.7556 - categorical_accuracy: 0.8868 - val_loss: 0.4573 - val_categorical_accuracy: 0.9642\n",
      "Epoch 28/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8008 - categorical_accuracy: 0.8611Epoch 00027: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.8012 - categorical_accuracy: 0.8610 - val_loss: 0.6763 - val_categorical_accuracy: 0.9589\n",
      "Epoch 29/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7228 - categorical_accuracy: 0.8757Epoch 00028: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.7228 - categorical_accuracy: 0.8757 - val_loss: 0.5209 - val_categorical_accuracy: 0.9504\n",
      "Epoch 30/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6860 - categorical_accuracy: 0.8842Epoch 00029: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6859 - categorical_accuracy: 0.8842 - val_loss: 0.4328 - val_categorical_accuracy: 0.9618\n",
      "Epoch 31/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6580 - categorical_accuracy: 0.8901Epoch 00030: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6577 - categorical_accuracy: 0.8902 - val_loss: 0.4070 - val_categorical_accuracy: 0.9666\n",
      "Epoch 32/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5573 - categorical_accuracy: 0.8930Epoch 00031: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5568 - categorical_accuracy: 0.8932 - val_loss: 0.3604 - val_categorical_accuracy: 0.9613\n",
      "Epoch 33/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4874 - categorical_accuracy: 0.8952Epoch 00032: val_loss improved from 0.34019 to 0.27062, saving model to /home/npasini1/Desktop/model_checkpoints/pyrecorder_Nicolo_1to16_allepochs_allsamples.h5\n",
      "37220/37220 [==============================] - 41s - loss: 0.4871 - categorical_accuracy: 0.8954 - val_loss: 0.2706 - val_categorical_accuracy: 0.9657\n",
      "Epoch 34/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4889 - categorical_accuracy: 0.8976Epoch 00033: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4892 - categorical_accuracy: 0.8977 - val_loss: 0.4140 - val_categorical_accuracy: 0.9669\n",
      "Epoch 35/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5756 - categorical_accuracy: 0.8956Epoch 00034: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5756 - categorical_accuracy: 0.8956 - val_loss: 0.4004 - val_categorical_accuracy: 0.9516\n",
      "Epoch 36/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6121 - categorical_accuracy: 0.8853Epoch 00035: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6114 - categorical_accuracy: 0.8855 - val_loss: 0.3435 - val_categorical_accuracy: 0.9674\n",
      "Epoch 37/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5241 - categorical_accuracy: 0.8933Epoch 00036: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5236 - categorical_accuracy: 0.8935 - val_loss: 0.3471 - val_categorical_accuracy: 0.9577\n",
      "Epoch 38/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5673 - categorical_accuracy: 0.9005Epoch 00037: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5683 - categorical_accuracy: 0.9005 - val_loss: 0.6315 - val_categorical_accuracy: 0.9695\n",
      "Epoch 39/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6981 - categorical_accuracy: 0.8870Epoch 00038: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6974 - categorical_accuracy: 0.8873 - val_loss: 0.3775 - val_categorical_accuracy: 0.9657\n",
      "Epoch 40/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6906 - categorical_accuracy: 0.8900Epoch 00039: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6903 - categorical_accuracy: 0.8902 - val_loss: 0.6136 - val_categorical_accuracy: 0.9572\n",
      "Epoch 41/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6891 - categorical_accuracy: 0.8905Epoch 00040: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6889 - categorical_accuracy: 0.8908 - val_loss: 0.5790 - val_categorical_accuracy: 0.9606\n",
      "Epoch 42/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6563 - categorical_accuracy: 0.8974Epoch 00041: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6559 - categorical_accuracy: 0.8976 - val_loss: 0.4387 - val_categorical_accuracy: 0.9691\n",
      "Epoch 43/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8737 - categorical_accuracy: 0.8919Epoch 00042: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.8734 - categorical_accuracy: 0.8920 - val_loss: 0.6075 - val_categorical_accuracy: 0.9635\n",
      "Epoch 44/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.0013 - categorical_accuracy: 0.8700Epoch 00043: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 1.0005 - categorical_accuracy: 0.8703 - val_loss: 0.6886 - val_categorical_accuracy: 0.9408\n",
      "Epoch 45/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6951 - categorical_accuracy: 0.8936Epoch 00044: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6945 - categorical_accuracy: 0.8938 - val_loss: 0.4501 - val_categorical_accuracy: 0.9565\n",
      "Epoch 46/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5658 - categorical_accuracy: 0.9030Epoch 00045: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5653 - categorical_accuracy: 0.9032 - val_loss: 0.3486 - val_categorical_accuracy: 0.9710\n",
      "Epoch 47/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.0720 - categorical_accuracy: 0.8761Epoch 00046: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 1.0716 - categorical_accuracy: 0.8762 - val_loss: 0.9376 - val_categorical_accuracy: 0.9543\n",
      "Epoch 48/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.0215 - categorical_accuracy: 0.8935Epoch 00047: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 1.0209 - categorical_accuracy: 0.8937 - val_loss: 0.7460 - val_categorical_accuracy: 0.9490\n",
      "Epoch 49/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7912 - categorical_accuracy: 0.9022Epoch 00048: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.7906 - categorical_accuracy: 0.9023 - val_loss: 0.5357 - val_categorical_accuracy: 0.9649\n",
      "Epoch 50/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8167 - categorical_accuracy: 0.8920Epoch 00049: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.8165 - categorical_accuracy: 0.8922 - val_loss: 0.6300 - val_categorical_accuracy: 0.9645\n",
      "Epoch 51/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.1175 - categorical_accuracy: 0.8168Epoch 00050: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 1.1185 - categorical_accuracy: 0.8167 - val_loss: 1.1179 - val_categorical_accuracy: 0.8578\n",
      "Epoch 52/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.5932 - categorical_accuracy: 0.8165Epoch 00051: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 1.5929 - categorical_accuracy: 0.8168 - val_loss: 1.3965 - val_categorical_accuracy: 0.9565\n",
      "Epoch 53/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.1623 - categorical_accuracy: 0.8770Epoch 00052: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 1.1614 - categorical_accuracy: 0.8773 - val_loss: 0.7245 - val_categorical_accuracy: 0.9606\n",
      "Epoch 54/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8673 - categorical_accuracy: 0.8887Epoch 00053: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.8668 - categorical_accuracy: 0.8889 - val_loss: 0.6264 - val_categorical_accuracy: 0.9623\n",
      "Epoch 55/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9161 - categorical_accuracy: 0.8945Epoch 00054: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.9155 - categorical_accuracy: 0.8946 - val_loss: 0.5915 - val_categorical_accuracy: 0.9630\n",
      "Epoch 56/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7225 - categorical_accuracy: 0.9009Epoch 00055: val_loss did not improve\n",
      "37220/37220 [==============================] - 44s - loss: 0.7220 - categorical_accuracy: 0.9011 - val_loss: 0.4648 - val_categorical_accuracy: 0.9630\n",
      "Epoch 57/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8435 - categorical_accuracy: 0.8955Epoch 00056: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.8449 - categorical_accuracy: 0.8956 - val_loss: 1.2701 - val_categorical_accuracy: 0.9620\n",
      "Epoch 58/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.1239 - categorical_accuracy: 0.8877Epoch 00057: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 1.1227 - categorical_accuracy: 0.8880 - val_loss: 0.6877 - val_categorical_accuracy: 0.9529\n",
      "Epoch 59/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7293 - categorical_accuracy: 0.9027Epoch 00058: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.7287 - categorical_accuracy: 0.9029 - val_loss: 0.5135 - val_categorical_accuracy: 0.9579\n",
      "Epoch 60/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9376 - categorical_accuracy: 0.8922Epoch 00059: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.9398 - categorical_accuracy: 0.8919 - val_loss: 1.4156 - val_categorical_accuracy: 0.8757\n",
      "Epoch 61/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.0730 - categorical_accuracy: 0.8761Epoch 00060: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 1.0720 - categorical_accuracy: 0.8763 - val_loss: 0.6441 - val_categorical_accuracy: 0.9463\n",
      "Epoch 62/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.8165 - categorical_accuracy: 0.8912Epoch 00061: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.8159 - categorical_accuracy: 0.8913 - val_loss: 0.5734 - val_categorical_accuracy: 0.9543\n",
      "Epoch 63/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7034 - categorical_accuracy: 0.9010Epoch 00062: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.7029 - categorical_accuracy: 0.9012 - val_loss: 0.4662 - val_categorical_accuracy: 0.9623\n",
      "Epoch 64/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5648 - categorical_accuracy: 0.9109Epoch 00063: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5645 - categorical_accuracy: 0.9110 - val_loss: 0.4057 - val_categorical_accuracy: 0.9620\n",
      "Epoch 65/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5432 - categorical_accuracy: 0.9151Epoch 00064: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5428 - categorical_accuracy: 0.9153 - val_loss: 0.4234 - val_categorical_accuracy: 0.9579\n",
      "Epoch 66/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5344 - categorical_accuracy: 0.9133Epoch 00065: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5341 - categorical_accuracy: 0.9134 - val_loss: 0.3973 - val_categorical_accuracy: 0.9606\n",
      "Epoch 67/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4930 - categorical_accuracy: 0.9169Epoch 00066: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4926 - categorical_accuracy: 0.9171 - val_loss: 0.3466 - val_categorical_accuracy: 0.9613\n",
      "Epoch 68/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4320 - categorical_accuracy: 0.9169Epoch 00067: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.4316 - categorical_accuracy: 0.9170 - val_loss: 0.3343 - val_categorical_accuracy: 0.9628\n",
      "Epoch 69/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4515 - categorical_accuracy: 0.9160Epoch 00068: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.4513 - categorical_accuracy: 0.9161 - val_loss: 0.3549 - val_categorical_accuracy: 0.8980\n",
      "Epoch 70/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5072 - categorical_accuracy: 0.9155Epoch 00069: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5070 - categorical_accuracy: 0.9157 - val_loss: 0.5046 - val_categorical_accuracy: 0.8922\n",
      "Epoch 71/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4958 - categorical_accuracy: 0.9131Epoch 00070: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.4953 - categorical_accuracy: 0.9133 - val_loss: 0.3932 - val_categorical_accuracy: 0.8965\n",
      "Epoch 72/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4128 - categorical_accuracy: 0.9210Epoch 00071: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.4124 - categorical_accuracy: 0.9212 - val_loss: 0.3942 - val_categorical_accuracy: 0.8968\n",
      "Epoch 73/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4435 - categorical_accuracy: 0.9191Epoch 00072: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4432 - categorical_accuracy: 0.9192 - val_loss: 0.3636 - val_categorical_accuracy: 0.8876\n",
      "Epoch 74/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4183 - categorical_accuracy: 0.9197Epoch 00073: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4180 - categorical_accuracy: 0.9198 - val_loss: 0.3685 - val_categorical_accuracy: 0.8958\n",
      "Epoch 75/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4520 - categorical_accuracy: 0.9144Epoch 00074: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.4520 - categorical_accuracy: 0.9145 - val_loss: 0.3918 - val_categorical_accuracy: 0.9664\n",
      "Epoch 76/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4542 - categorical_accuracy: 0.9184Epoch 00075: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4543 - categorical_accuracy: 0.9186 - val_loss: 0.5978 - val_categorical_accuracy: 0.8994\n",
      "Epoch 77/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6211 - categorical_accuracy: 0.9041Epoch 00076: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.6204 - categorical_accuracy: 0.9043 - val_loss: 0.3709 - val_categorical_accuracy: 0.9630\n",
      "Epoch 78/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4837 - categorical_accuracy: 0.9152Epoch 00077: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.4834 - categorical_accuracy: 0.9153 - val_loss: 0.3764 - val_categorical_accuracy: 0.9545\n",
      "Epoch 79/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4317 - categorical_accuracy: 0.9172Epoch 00078: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4313 - categorical_accuracy: 0.9173 - val_loss: 0.3496 - val_categorical_accuracy: 0.8888\n",
      "Epoch 80/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3583 - categorical_accuracy: 0.9261Epoch 00079: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3582 - categorical_accuracy: 0.9262 - val_loss: 0.3385 - val_categorical_accuracy: 0.8895\n",
      "Epoch 81/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3367 - categorical_accuracy: 0.9304Epoch 00080: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3364 - categorical_accuracy: 0.9306 - val_loss: 0.3018 - val_categorical_accuracy: 0.8914\n",
      "Epoch 82/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3161 - categorical_accuracy: 0.9330Epoch 00081: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3159 - categorical_accuracy: 0.9331 - val_loss: 0.2997 - val_categorical_accuracy: 0.8924\n",
      "Epoch 83/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3298 - categorical_accuracy: 0.9333Epoch 00082: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3295 - categorical_accuracy: 0.9334 - val_loss: 0.2835 - val_categorical_accuracy: 0.8924\n",
      "Epoch 84/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3199 - categorical_accuracy: 0.9327Epoch 00083: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3197 - categorical_accuracy: 0.9328 - val_loss: 0.2928 - val_categorical_accuracy: 0.8948\n",
      "Epoch 85/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3231 - categorical_accuracy: 0.9331Epoch 00084: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3228 - categorical_accuracy: 0.9333 - val_loss: 0.3571 - val_categorical_accuracy: 0.8893\n",
      "Epoch 86/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3284 - categorical_accuracy: 0.9358Epoch 00085: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3281 - categorical_accuracy: 0.9359 - val_loss: 0.2956 - val_categorical_accuracy: 0.8924\n",
      "Epoch 87/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3077 - categorical_accuracy: 0.9341Epoch 00086: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3074 - categorical_accuracy: 0.9342 - val_loss: 0.2997 - val_categorical_accuracy: 0.8905\n",
      "Epoch 88/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3084 - categorical_accuracy: 0.9332Epoch 00087: val_loss improved from 0.27062 to 0.26333, saving model to /home/npasini1/Desktop/model_checkpoints/pyrecorder_Nicolo_1to16_allepochs_allsamples.h5\n",
      "37220/37220 [==============================] - 41s - loss: 0.3081 - categorical_accuracy: 0.9334 - val_loss: 0.2633 - val_categorical_accuracy: 0.8948\n",
      "Epoch 89/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2887 - categorical_accuracy: 0.9353Epoch 00088: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2886 - categorical_accuracy: 0.9354 - val_loss: 0.2983 - val_categorical_accuracy: 0.8963\n",
      "Epoch 90/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3101 - categorical_accuracy: 0.9370Epoch 00089: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3099 - categorical_accuracy: 0.9372 - val_loss: 0.2864 - val_categorical_accuracy: 0.9613\n",
      "Epoch 91/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3125 - categorical_accuracy: 0.9374Epoch 00090: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3124 - categorical_accuracy: 0.9375 - val_loss: 0.3365 - val_categorical_accuracy: 0.8885\n",
      "Epoch 92/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3403 - categorical_accuracy: 0.9319Epoch 00091: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3399 - categorical_accuracy: 0.9321 - val_loss: 0.3210 - val_categorical_accuracy: 0.8847\n",
      "Epoch 93/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3234 - categorical_accuracy: 0.9378Epoch 00092: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3231 - categorical_accuracy: 0.9379 - val_loss: 0.2988 - val_categorical_accuracy: 0.8907\n",
      "Epoch 94/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2930 - categorical_accuracy: 0.9406Epoch 00093: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2927 - categorical_accuracy: 0.9407 - val_loss: 0.3275 - val_categorical_accuracy: 0.8888\n",
      "Epoch 95/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2903 - categorical_accuracy: 0.9394Epoch 00094: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2901 - categorical_accuracy: 0.9395 - val_loss: 0.2920 - val_categorical_accuracy: 0.8922\n",
      "Epoch 96/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2839 - categorical_accuracy: 0.9416Epoch 00095: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2835 - categorical_accuracy: 0.9417 - val_loss: 0.2823 - val_categorical_accuracy: 0.8919\n",
      "Epoch 97/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6459 - categorical_accuracy: 0.9139Epoch 00096: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6461 - categorical_accuracy: 0.9141 - val_loss: 0.6356 - val_categorical_accuracy: 0.9681\n",
      "Epoch 98/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9937 - categorical_accuracy: 0.9192Epoch 00097: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.9924 - categorical_accuracy: 0.9193 - val_loss: 0.6284 - val_categorical_accuracy: 0.8936\n",
      "Epoch 99/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5427 - categorical_accuracy: 0.9369Epoch 00098: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5421 - categorical_accuracy: 0.9370 - val_loss: 0.4692 - val_categorical_accuracy: 0.8931\n",
      "Epoch 100/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4402 - categorical_accuracy: 0.9391Epoch 00099: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4397 - categorical_accuracy: 0.9392 - val_loss: 0.4175 - val_categorical_accuracy: 0.8907\n",
      "Epoch 101/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3911 - categorical_accuracy: 0.9394Epoch 00100: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3908 - categorical_accuracy: 0.9394 - val_loss: 0.3919 - val_categorical_accuracy: 0.8936\n",
      "Epoch 102/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3477 - categorical_accuracy: 0.9437Epoch 00101: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3474 - categorical_accuracy: 0.9438 - val_loss: 0.3730 - val_categorical_accuracy: 0.8929\n",
      "Epoch 103/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3516 - categorical_accuracy: 0.9421Epoch 00102: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3514 - categorical_accuracy: 0.9422 - val_loss: 0.3743 - val_categorical_accuracy: 0.8902\n",
      "Epoch 104/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3076 - categorical_accuracy: 0.9449Epoch 00103: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3073 - categorical_accuracy: 0.9450 - val_loss: 0.3331 - val_categorical_accuracy: 0.8912\n",
      "Epoch 105/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4001 - categorical_accuracy: 0.9400Epoch 00104: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3995 - categorical_accuracy: 0.9401 - val_loss: 0.3398 - val_categorical_accuracy: 0.8907\n",
      "Epoch 106/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3600 - categorical_accuracy: 0.9398Epoch 00105: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3597 - categorical_accuracy: 0.9400 - val_loss: 0.3514 - val_categorical_accuracy: 0.8926\n",
      "Epoch 107/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3294 - categorical_accuracy: 0.9420Epoch 00106: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3291 - categorical_accuracy: 0.9420 - val_loss: 0.3302 - val_categorical_accuracy: 0.8922\n",
      "Epoch 108/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3005 - categorical_accuracy: 0.9449Epoch 00107: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3002 - categorical_accuracy: 0.9450 - val_loss: 0.3246 - val_categorical_accuracy: 0.8934\n",
      "Epoch 109/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2797 - categorical_accuracy: 0.9442Epoch 00108: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2795 - categorical_accuracy: 0.9443 - val_loss: 0.3282 - val_categorical_accuracy: 0.8861\n",
      "Epoch 110/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2863 - categorical_accuracy: 0.9447Epoch 00109: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2862 - categorical_accuracy: 0.9448 - val_loss: 0.3023 - val_categorical_accuracy: 0.8893\n",
      "Epoch 111/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2637 - categorical_accuracy: 0.9461Epoch 00110: val_loss did not improve\n",
      "37220/37220 [==============================] - 45s - loss: 0.2634 - categorical_accuracy: 0.9462 - val_loss: 0.2961 - val_categorical_accuracy: 0.8902\n",
      "Epoch 112/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2585 - categorical_accuracy: 0.9442Epoch 00111: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2582 - categorical_accuracy: 0.9443 - val_loss: 0.2717 - val_categorical_accuracy: 0.8912\n",
      "Epoch 113/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2813 - categorical_accuracy: 0.9436Epoch 00112: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2810 - categorical_accuracy: 0.9437 - val_loss: 0.3003 - val_categorical_accuracy: 0.8912\n",
      "Epoch 114/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3699 - categorical_accuracy: 0.9450Epoch 00113: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3728 - categorical_accuracy: 0.9451 - val_loss: 1.6362 - val_categorical_accuracy: 0.8897\n",
      "Epoch 115/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.1122 - categorical_accuracy: 0.9256Epoch 00114: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 1.1109 - categorical_accuracy: 0.9258 - val_loss: 0.7617 - val_categorical_accuracy: 0.8774\n",
      "Epoch 116/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6025 - categorical_accuracy: 0.9429Epoch 00115: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.6025 - categorical_accuracy: 0.9429 - val_loss: 0.5564 - val_categorical_accuracy: 0.8856\n",
      "Epoch 117/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4713 - categorical_accuracy: 0.9444Epoch 00116: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.4709 - categorical_accuracy: 0.9445 - val_loss: 0.4996 - val_categorical_accuracy: 0.8859\n",
      "Epoch 118/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4408 - categorical_accuracy: 0.9430Epoch 00117: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.4408 - categorical_accuracy: 0.9430 - val_loss: 0.4216 - val_categorical_accuracy: 0.9456\n",
      "Epoch 119/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3730 - categorical_accuracy: 0.9446Epoch 00118: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.3727 - categorical_accuracy: 0.9447 - val_loss: 0.3484 - val_categorical_accuracy: 0.9558\n",
      "Epoch 120/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3344 - categorical_accuracy: 0.9475Epoch 00119: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.3341 - categorical_accuracy: 0.9476 - val_loss: 0.3314 - val_categorical_accuracy: 0.8897\n",
      "Epoch 121/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3027 - categorical_accuracy: 0.9471Epoch 00120: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.3025 - categorical_accuracy: 0.9472 - val_loss: 0.3118 - val_categorical_accuracy: 0.8914\n",
      "Epoch 122/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5446 - categorical_accuracy: 0.9384Epoch 00121: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.5445 - categorical_accuracy: 0.9384 - val_loss: 0.4947 - val_categorical_accuracy: 0.9545\n",
      "Epoch 123/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6761 - categorical_accuracy: 0.9291Epoch 00122: val_loss did not improve\n",
      "37220/37220 [==============================] - 47s - loss: 0.6762 - categorical_accuracy: 0.9292 - val_loss: 0.7600 - val_categorical_accuracy: 0.9632\n",
      "Epoch 124/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6739 - categorical_accuracy: 0.9266Epoch 00123: val_loss did not improve\n",
      "37220/37220 [==============================] - 48s - loss: 0.6741 - categorical_accuracy: 0.9266 - val_loss: 0.4801 - val_categorical_accuracy: 0.9596\n",
      "Epoch 125/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.1491 - categorical_accuracy: 0.9297Epoch 00124: val_loss did not improve\n",
      "37220/37220 [==============================] - 50s - loss: 1.1525 - categorical_accuracy: 0.9298 - val_loss: 2.5108 - val_categorical_accuracy: 0.9234\n",
      "Epoch 126/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 2.0810 - categorical_accuracy: 0.9237Epoch 00125: val_loss did not improve\n",
      "37220/37220 [==============================] - 54s - loss: 2.0796 - categorical_accuracy: 0.9238 - val_loss: 1.7133 - val_categorical_accuracy: 0.8946\n",
      "Epoch 127/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.5473 - categorical_accuracy: 0.9313Epoch 00126: val_loss did not improve\n",
      "37220/37220 [==============================] - 51s - loss: 1.5465 - categorical_accuracy: 0.9314 - val_loss: 1.3367 - val_categorical_accuracy: 0.9260\n",
      "Epoch 128/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.2207 - categorical_accuracy: 0.9394Epoch 00127: val_loss did not improve\n",
      "37220/37220 [==============================] - 50s - loss: 1.2201 - categorical_accuracy: 0.9395 - val_loss: 1.1011 - val_categorical_accuracy: 0.8895\n",
      "Epoch 129/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 1.0369 - categorical_accuracy: 0.9371Epoch 00128: val_loss did not improve\n",
      "37220/37220 [==============================] - 50s - loss: 1.0365 - categorical_accuracy: 0.9373 - val_loss: 1.0126 - val_categorical_accuracy: 0.8905\n",
      "Epoch 130/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9064 - categorical_accuracy: 0.9390Epoch 00129: val_loss did not improve\n",
      "37220/37220 [==============================] - 50s - loss: 0.9061 - categorical_accuracy: 0.9391 - val_loss: 0.8289 - val_categorical_accuracy: 0.9545\n",
      "Epoch 131/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7965 - categorical_accuracy: 0.9413Epoch 00130: val_loss did not improve\n",
      "37220/37220 [==============================] - 50s - loss: 0.7966 - categorical_accuracy: 0.9414 - val_loss: 0.9848 - val_categorical_accuracy: 0.8866\n",
      "Epoch 132/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.9807 - categorical_accuracy: 0.9324Epoch 00131: val_loss did not improve\n",
      "37220/37220 [==============================] - 50s - loss: 0.9798 - categorical_accuracy: 0.9325 - val_loss: 0.7938 - val_categorical_accuracy: 0.9616\n",
      "Epoch 133/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7637 - categorical_accuracy: 0.9402Epoch 00132: val_loss did not improve\n",
      "37220/37220 [==============================] - 47s - loss: 0.7635 - categorical_accuracy: 0.9402 - val_loss: 0.7293 - val_categorical_accuracy: 0.9678\n",
      "Epoch 134/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.7019 - categorical_accuracy: 0.9394Epoch 00133: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.7014 - categorical_accuracy: 0.9395 - val_loss: 0.6258 - val_categorical_accuracy: 0.9596\n",
      "Epoch 135/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.6073 - categorical_accuracy: 0.9412Epoch 00134: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.6071 - categorical_accuracy: 0.9413 - val_loss: 0.5981 - val_categorical_accuracy: 0.9623\n",
      "Epoch 136/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5716 - categorical_accuracy: 0.9440Epoch 00135: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5713 - categorical_accuracy: 0.9441 - val_loss: 0.5678 - val_categorical_accuracy: 0.9625\n",
      "Epoch 137/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5356 - categorical_accuracy: 0.9415Epoch 00136: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.5353 - categorical_accuracy: 0.9415 - val_loss: 0.5434 - val_categorical_accuracy: 0.9623\n",
      "Epoch 138/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5094 - categorical_accuracy: 0.9441Epoch 00137: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.5092 - categorical_accuracy: 0.9441 - val_loss: 0.5240 - val_categorical_accuracy: 0.9623\n",
      "Epoch 139/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4925 - categorical_accuracy: 0.9429Epoch 00138: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.4921 - categorical_accuracy: 0.9430 - val_loss: 0.5015 - val_categorical_accuracy: 0.9601\n",
      "Epoch 140/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4780 - categorical_accuracy: 0.9392Epoch 00139: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.4778 - categorical_accuracy: 0.9393 - val_loss: 0.4828 - val_categorical_accuracy: 0.9616\n",
      "Epoch 141/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4600 - categorical_accuracy: 0.9408Epoch 00140: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.4597 - categorical_accuracy: 0.9410 - val_loss: 0.4701 - val_categorical_accuracy: 0.9512\n",
      "Epoch 142/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4334 - categorical_accuracy: 0.9450Epoch 00141: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4332 - categorical_accuracy: 0.9451 - val_loss: 0.4665 - val_categorical_accuracy: 0.8926\n",
      "Epoch 143/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4307 - categorical_accuracy: 0.9418Epoch 00142: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4305 - categorical_accuracy: 0.9420 - val_loss: 0.4642 - val_categorical_accuracy: 0.8910\n",
      "Epoch 144/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4027 - categorical_accuracy: 0.9475Epoch 00143: val_loss did not improve\n",
      "37220/37220 [==============================] - 44s - loss: 0.4026 - categorical_accuracy: 0.9476 - val_loss: 0.4450 - val_categorical_accuracy: 0.8922\n",
      "Epoch 145/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3942 - categorical_accuracy: 0.9446Epoch 00144: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.3940 - categorical_accuracy: 0.9447 - val_loss: 0.4209 - val_categorical_accuracy: 0.8943\n",
      "Epoch 146/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3813 - categorical_accuracy: 0.9451Epoch 00145: val_loss did not improve\n",
      "37220/37220 [==============================] - 44s - loss: 0.3811 - categorical_accuracy: 0.9452 - val_loss: 0.4006 - val_categorical_accuracy: 0.9596\n",
      "Epoch 147/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3680 - categorical_accuracy: 0.9473Epoch 00146: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3678 - categorical_accuracy: 0.9473 - val_loss: 0.4020 - val_categorical_accuracy: 0.8926\n",
      "Epoch 148/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3806 - categorical_accuracy: 0.9485Epoch 00147: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3808 - categorical_accuracy: 0.9487 - val_loss: 0.5452 - val_categorical_accuracy: 0.9618\n",
      "Epoch 149/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4271 - categorical_accuracy: 0.9473Epoch 00148: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4267 - categorical_accuracy: 0.9474 - val_loss: 0.4226 - val_categorical_accuracy: 0.8934\n",
      "Epoch 150/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3735 - categorical_accuracy: 0.9409Epoch 00149: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3732 - categorical_accuracy: 0.9410 - val_loss: 0.4149 - val_categorical_accuracy: 0.8919\n",
      "Epoch 151/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3448 - categorical_accuracy: 0.9424Epoch 00150: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3445 - categorical_accuracy: 0.9425 - val_loss: 0.3872 - val_categorical_accuracy: 0.8910\n",
      "Epoch 152/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3360 - categorical_accuracy: 0.9493Epoch 00151: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.3359 - categorical_accuracy: 0.9494 - val_loss: 0.3821 - val_categorical_accuracy: 0.8914\n",
      "Epoch 153/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3241 - categorical_accuracy: 0.9440Epoch 00152: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.3238 - categorical_accuracy: 0.9441 - val_loss: 0.3748 - val_categorical_accuracy: 0.8941\n",
      "Epoch 154/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3282 - categorical_accuracy: 0.9425Epoch 00153: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.3281 - categorical_accuracy: 0.9426 - val_loss: 0.3648 - val_categorical_accuracy: 0.8953\n",
      "Epoch 155/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4411 - categorical_accuracy: 0.9440Epoch 00154: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.4417 - categorical_accuracy: 0.9441 - val_loss: 0.6991 - val_categorical_accuracy: 0.8946\n",
      "Epoch 156/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.5336 - categorical_accuracy: 0.9335Epoch 00155: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.5333 - categorical_accuracy: 0.9336 - val_loss: 0.5059 - val_categorical_accuracy: 0.8917\n",
      "Epoch 157/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4067 - categorical_accuracy: 0.9409Epoch 00156: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.4066 - categorical_accuracy: 0.9410 - val_loss: 0.4455 - val_categorical_accuracy: 0.8919\n",
      "Epoch 158/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3646 - categorical_accuracy: 0.9456Epoch 00157: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3650 - categorical_accuracy: 0.9456 - val_loss: 0.5219 - val_categorical_accuracy: 0.8982\n",
      "Epoch 159/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4146 - categorical_accuracy: 0.9378Epoch 00158: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.4146 - categorical_accuracy: 0.9379 - val_loss: 0.4066 - val_categorical_accuracy: 0.9468\n",
      "Epoch 160/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3442 - categorical_accuracy: 0.9454Epoch 00159: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3440 - categorical_accuracy: 0.9455 - val_loss: 0.3780 - val_categorical_accuracy: 0.9313\n",
      "Epoch 161/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3299 - categorical_accuracy: 0.9470Epoch 00160: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.3300 - categorical_accuracy: 0.9470 - val_loss: 0.3830 - val_categorical_accuracy: 0.8939\n",
      "Epoch 162/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3216 - categorical_accuracy: 0.9417Epoch 00161: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.3214 - categorical_accuracy: 0.9418 - val_loss: 0.3674 - val_categorical_accuracy: 0.8965\n",
      "Epoch 163/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2994 - categorical_accuracy: 0.9430Epoch 00162: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2992 - categorical_accuracy: 0.9430 - val_loss: 0.3597 - val_categorical_accuracy: 0.8951\n",
      "Epoch 164/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2964 - categorical_accuracy: 0.9424Epoch 00163: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.2962 - categorical_accuracy: 0.9425 - val_loss: 0.3499 - val_categorical_accuracy: 0.8965\n",
      "Epoch 165/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2861 - categorical_accuracy: 0.9471Epoch 00164: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.2860 - categorical_accuracy: 0.9472 - val_loss: 0.3435 - val_categorical_accuracy: 0.8943\n",
      "Epoch 166/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2833 - categorical_accuracy: 0.9484Epoch 00165: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2833 - categorical_accuracy: 0.9485 - val_loss: 0.3375 - val_categorical_accuracy: 0.9144\n",
      "Epoch 167/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2725 - categorical_accuracy: 0.9447Epoch 00166: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.2723 - categorical_accuracy: 0.9448 - val_loss: 0.3198 - val_categorical_accuracy: 0.9572\n",
      "Epoch 168/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2774 - categorical_accuracy: 0.9418Epoch 00167: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2772 - categorical_accuracy: 0.9419 - val_loss: 0.3187 - val_categorical_accuracy: 0.9550\n",
      "Epoch 169/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.4419 - categorical_accuracy: 0.9415Epoch 00168: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.4416 - categorical_accuracy: 0.9417 - val_loss: 0.4833 - val_categorical_accuracy: 0.9115\n",
      "Epoch 170/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3952 - categorical_accuracy: 0.9408Epoch 00169: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.3950 - categorical_accuracy: 0.9409 - val_loss: 0.3930 - val_categorical_accuracy: 0.9151\n",
      "Epoch 171/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3270 - categorical_accuracy: 0.9418Epoch 00170: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3267 - categorical_accuracy: 0.9419 - val_loss: 0.3754 - val_categorical_accuracy: 0.8956\n",
      "Epoch 172/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2953 - categorical_accuracy: 0.9441Epoch 00171: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.2953 - categorical_accuracy: 0.9441 - val_loss: 0.3501 - val_categorical_accuracy: 0.8968\n",
      "Epoch 173/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2878 - categorical_accuracy: 0.9464Epoch 00172: val_loss did not improve\n",
      "37220/37220 [==============================] - 40s - loss: 0.2876 - categorical_accuracy: 0.9466 - val_loss: 0.3387 - val_categorical_accuracy: 0.8963\n",
      "Epoch 174/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2997 - categorical_accuracy: 0.9416Epoch 00173: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2994 - categorical_accuracy: 0.9417 - val_loss: 0.3184 - val_categorical_accuracy: 0.9666\n",
      "Epoch 175/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3054 - categorical_accuracy: 0.9475Epoch 00174: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.3054 - categorical_accuracy: 0.9476 - val_loss: 0.3580 - val_categorical_accuracy: 0.9250\n",
      "Epoch 176/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2626 - categorical_accuracy: 0.9503Epoch 00175: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2623 - categorical_accuracy: 0.9505 - val_loss: 0.3262 - val_categorical_accuracy: 0.9301\n",
      "Epoch 177/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2623 - categorical_accuracy: 0.9467Epoch 00176: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2621 - categorical_accuracy: 0.9468 - val_loss: 0.3259 - val_categorical_accuracy: 0.9584\n",
      "Epoch 178/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2572 - categorical_accuracy: 0.9489Epoch 00177: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2572 - categorical_accuracy: 0.9490 - val_loss: 0.3271 - val_categorical_accuracy: 0.9289\n",
      "Epoch 179/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2636 - categorical_accuracy: 0.9449Epoch 00178: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2633 - categorical_accuracy: 0.9450 - val_loss: 0.3192 - val_categorical_accuracy: 0.9616\n",
      "Epoch 180/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2407 - categorical_accuracy: 0.9508Epoch 00179: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2405 - categorical_accuracy: 0.9509 - val_loss: 0.3085 - val_categorical_accuracy: 0.9611\n",
      "Epoch 181/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2390 - categorical_accuracy: 0.9505Epoch 00180: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2388 - categorical_accuracy: 0.9506 - val_loss: 0.3117 - val_categorical_accuracy: 0.9507\n",
      "Epoch 182/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2260 - categorical_accuracy: 0.9546Epoch 00181: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2258 - categorical_accuracy: 0.9547 - val_loss: 0.3154 - val_categorical_accuracy: 0.8939\n",
      "Epoch 183/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2360 - categorical_accuracy: 0.9534Epoch 00182: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2359 - categorical_accuracy: 0.9534 - val_loss: 0.3150 - val_categorical_accuracy: 0.9110\n",
      "Epoch 184/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2276 - categorical_accuracy: 0.9522Epoch 00183: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2275 - categorical_accuracy: 0.9523 - val_loss: 0.3124 - val_categorical_accuracy: 0.9437\n",
      "Epoch 185/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2224 - categorical_accuracy: 0.9534Epoch 00184: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2222 - categorical_accuracy: 0.9535 - val_loss: 0.3134 - val_categorical_accuracy: 0.8948\n",
      "Epoch 186/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2178 - categorical_accuracy: 0.9542Epoch 00185: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2179 - categorical_accuracy: 0.9542 - val_loss: 0.3085 - val_categorical_accuracy: 0.8960\n",
      "Epoch 187/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2280 - categorical_accuracy: 0.9471Epoch 00186: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2278 - categorical_accuracy: 0.9472 - val_loss: 0.3134 - val_categorical_accuracy: 0.8958\n",
      "Epoch 188/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2139 - categorical_accuracy: 0.9520Epoch 00187: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2137 - categorical_accuracy: 0.9521 - val_loss: 0.3163 - val_categorical_accuracy: 0.8951\n",
      "Epoch 189/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2187 - categorical_accuracy: 0.9546Epoch 00188: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2187 - categorical_accuracy: 0.9546 - val_loss: 0.3233 - val_categorical_accuracy: 0.8941\n",
      "Epoch 190/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2256 - categorical_accuracy: 0.9515Epoch 00189: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2254 - categorical_accuracy: 0.9515 - val_loss: 0.3128 - val_categorical_accuracy: 0.8953\n",
      "Epoch 191/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2201 - categorical_accuracy: 0.9529Epoch 00190: val_loss did not improve\n",
      "37220/37220 [==============================] - 44s - loss: 0.2200 - categorical_accuracy: 0.9529 - val_loss: 0.3071 - val_categorical_accuracy: 0.8943\n",
      "Epoch 192/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2094 - categorical_accuracy: 0.9553Epoch 00191: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2094 - categorical_accuracy: 0.9553 - val_loss: 0.3050 - val_categorical_accuracy: 0.8953\n",
      "Epoch 193/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2180 - categorical_accuracy: 0.9555Epoch 00192: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.2178 - categorical_accuracy: 0.9556 - val_loss: 0.3254 - val_categorical_accuracy: 0.8934\n",
      "Epoch 194/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2149 - categorical_accuracy: 0.9541Epoch 00193: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2148 - categorical_accuracy: 0.9542 - val_loss: 0.3103 - val_categorical_accuracy: 0.8931\n",
      "Epoch 195/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2078 - categorical_accuracy: 0.9563Epoch 00194: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2077 - categorical_accuracy: 0.9563 - val_loss: 0.3178 - val_categorical_accuracy: 0.8926\n",
      "Epoch 196/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2109 - categorical_accuracy: 0.9518Epoch 00195: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2108 - categorical_accuracy: 0.9519 - val_loss: 0.3045 - val_categorical_accuracy: 0.8931\n",
      "Epoch 197/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2079 - categorical_accuracy: 0.9506Epoch 00196: val_loss did not improve\n",
      "37220/37220 [==============================] - 44s - loss: 0.2078 - categorical_accuracy: 0.9507 - val_loss: 0.3096 - val_categorical_accuracy: 0.8943\n",
      "Epoch 198/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2192 - categorical_accuracy: 0.9527Epoch 00197: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.2190 - categorical_accuracy: 0.9528 - val_loss: 0.3290 - val_categorical_accuracy: 0.8931\n",
      "Epoch 199/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3679 - categorical_accuracy: 0.9464Epoch 00198: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.3677 - categorical_accuracy: 0.9465 - val_loss: 0.4679 - val_categorical_accuracy: 0.8965\n",
      "Epoch 200/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3219 - categorical_accuracy: 0.9507Epoch 00199: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.3216 - categorical_accuracy: 0.9508 - val_loss: 0.3838 - val_categorical_accuracy: 0.8970\n",
      "Epoch 201/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2663 - categorical_accuracy: 0.9506Epoch 00200: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2661 - categorical_accuracy: 0.9507 - val_loss: 0.3408 - val_categorical_accuracy: 0.8985\n",
      "Epoch 202/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2376 - categorical_accuracy: 0.9567Epoch 00201: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2375 - categorical_accuracy: 0.9567 - val_loss: 0.3343 - val_categorical_accuracy: 0.8980\n",
      "Epoch 203/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2249 - categorical_accuracy: 0.9571Epoch 00202: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2247 - categorical_accuracy: 0.9571 - val_loss: 0.3082 - val_categorical_accuracy: 0.8968\n",
      "Epoch 204/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2254 - categorical_accuracy: 0.9542Epoch 00203: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2252 - categorical_accuracy: 0.9542 - val_loss: 0.3307 - val_categorical_accuracy: 0.8951\n",
      "Epoch 205/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2180 - categorical_accuracy: 0.9603Epoch 00204: val_loss did not improve\n",
      "37220/37220 [==============================] - 47s - loss: 0.2178 - categorical_accuracy: 0.9604 - val_loss: 0.3394 - val_categorical_accuracy: 0.8953\n",
      "Epoch 206/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2128 - categorical_accuracy: 0.9548Epoch 00205: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.2127 - categorical_accuracy: 0.9549 - val_loss: 0.3296 - val_categorical_accuracy: 0.8943\n",
      "Epoch 207/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2080 - categorical_accuracy: 0.9551Epoch 00206: val_loss did not improve\n",
      "37220/37220 [==============================] - 44s - loss: 0.2078 - categorical_accuracy: 0.9552 - val_loss: 0.3092 - val_categorical_accuracy: 0.8980\n",
      "Epoch 208/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1980 - categorical_accuracy: 0.9569Epoch 00207: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.1977 - categorical_accuracy: 0.9570 - val_loss: 0.3150 - val_categorical_accuracy: 0.8975\n",
      "Epoch 209/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3791 - categorical_accuracy: 0.9521Epoch 00208: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.3792 - categorical_accuracy: 0.9522 - val_loss: 0.4782 - val_categorical_accuracy: 0.8982\n",
      "Epoch 210/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3482 - categorical_accuracy: 0.9493Epoch 00209: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.3480 - categorical_accuracy: 0.9494 - val_loss: 0.3957 - val_categorical_accuracy: 0.8982\n",
      "Epoch 211/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2829 - categorical_accuracy: 0.9556Epoch 00210: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.2827 - categorical_accuracy: 0.9556 - val_loss: 0.3697 - val_categorical_accuracy: 0.8980\n",
      "Epoch 212/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2642 - categorical_accuracy: 0.9565Epoch 00211: val_loss did not improve\n",
      "37220/37220 [==============================] - 44s - loss: 0.2640 - categorical_accuracy: 0.9565 - val_loss: 0.3702 - val_categorical_accuracy: 0.8980\n",
      "Epoch 213/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3069 - categorical_accuracy: 0.9537Epoch 00212: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.3069 - categorical_accuracy: 0.9538 - val_loss: 0.4905 - val_categorical_accuracy: 0.8989\n",
      "Epoch 214/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.3080 - categorical_accuracy: 0.9526Epoch 00213: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.3077 - categorical_accuracy: 0.9526 - val_loss: 0.4330 - val_categorical_accuracy: 0.9001\n",
      "Epoch 215/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2691 - categorical_accuracy: 0.9474Epoch 00214: val_loss did not improve\n",
      "37220/37220 [==============================] - 45s - loss: 0.2689 - categorical_accuracy: 0.9474 - val_loss: 0.3931 - val_categorical_accuracy: 0.8994\n",
      "Epoch 216/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2464 - categorical_accuracy: 0.9559Epoch 00215: val_loss did not improve\n",
      "37220/37220 [==============================] - 43s - loss: 0.2466 - categorical_accuracy: 0.9560 - val_loss: 0.3694 - val_categorical_accuracy: 0.8997\n",
      "Epoch 217/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2318 - categorical_accuracy: 0.9535Epoch 00216: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2322 - categorical_accuracy: 0.9536 - val_loss: 0.3582 - val_categorical_accuracy: 0.8994\n",
      "Epoch 218/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2315 - categorical_accuracy: 0.9541Epoch 00217: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2313 - categorical_accuracy: 0.9542 - val_loss: 0.3575 - val_categorical_accuracy: 0.8994\n",
      "Epoch 219/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2806 - categorical_accuracy: 0.9551Epoch 00218: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2805 - categorical_accuracy: 0.9552 - val_loss: 0.3945 - val_categorical_accuracy: 0.8985\n",
      "Epoch 220/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2554 - categorical_accuracy: 0.9558Epoch 00219: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2555 - categorical_accuracy: 0.9559 - val_loss: 0.3662 - val_categorical_accuracy: 0.8987\n",
      "Epoch 221/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2280 - categorical_accuracy: 0.9557Epoch 00220: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2282 - categorical_accuracy: 0.9556 - val_loss: 0.3624 - val_categorical_accuracy: 0.8989\n",
      "Epoch 222/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2169 - categorical_accuracy: 0.9551Epoch 00221: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2169 - categorical_accuracy: 0.9551 - val_loss: 0.3588 - val_categorical_accuracy: 0.8985\n",
      "Epoch 223/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2186 - categorical_accuracy: 0.9565Epoch 00222: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2185 - categorical_accuracy: 0.9565 - val_loss: 0.3814 - val_categorical_accuracy: 0.8951\n",
      "Epoch 224/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2298 - categorical_accuracy: 0.9553Epoch 00223: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2295 - categorical_accuracy: 0.9554 - val_loss: 0.3646 - val_categorical_accuracy: 0.8975\n",
      "Epoch 225/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2104 - categorical_accuracy: 0.9556Epoch 00224: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2103 - categorical_accuracy: 0.9557 - val_loss: 0.3629 - val_categorical_accuracy: 0.8982\n",
      "Epoch 226/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2090 - categorical_accuracy: 0.9568Epoch 00225: val_loss did not improve\n",
      "37220/37220 [==============================] - 42s - loss: 0.2088 - categorical_accuracy: 0.9569 - val_loss: 0.3640 - val_categorical_accuracy: 0.8989\n",
      "Epoch 227/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2244 - categorical_accuracy: 0.9572Epoch 00226: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2243 - categorical_accuracy: 0.9572 - val_loss: 0.3905 - val_categorical_accuracy: 0.8972\n",
      "Epoch 228/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2217 - categorical_accuracy: 0.9530Epoch 00227: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2233 - categorical_accuracy: 0.9518 - val_loss: 0.3556 - val_categorical_accuracy: 0.8975\n",
      "Epoch 229/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2078 - categorical_accuracy: 0.9577Epoch 00228: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2076 - categorical_accuracy: 0.9577 - val_loss: 0.4010 - val_categorical_accuracy: 0.8985\n",
      "Epoch 230/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2313 - categorical_accuracy: 0.9584Epoch 00229: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2310 - categorical_accuracy: 0.9585 - val_loss: 0.3665 - val_categorical_accuracy: 0.8987\n",
      "Epoch 231/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2128 - categorical_accuracy: 0.9579Epoch 00230: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2129 - categorical_accuracy: 0.9580 - val_loss: 0.3470 - val_categorical_accuracy: 0.8982\n",
      "Epoch 232/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2049 - categorical_accuracy: 0.9582Epoch 00231: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2048 - categorical_accuracy: 0.9582 - val_loss: 0.3392 - val_categorical_accuracy: 0.8985\n",
      "Epoch 233/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1939 - categorical_accuracy: 0.9615Epoch 00232: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1939 - categorical_accuracy: 0.9616 - val_loss: 0.3384 - val_categorical_accuracy: 0.8982\n",
      "Epoch 234/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1943 - categorical_accuracy: 0.9584Epoch 00233: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1941 - categorical_accuracy: 0.9585 - val_loss: 0.3411 - val_categorical_accuracy: 0.8980\n",
      "Epoch 235/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1863 - categorical_accuracy: 0.9596Epoch 00234: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1861 - categorical_accuracy: 0.9597 - val_loss: 0.3349 - val_categorical_accuracy: 0.8970\n",
      "Epoch 236/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1908 - categorical_accuracy: 0.9601Epoch 00235: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1907 - categorical_accuracy: 0.9601 - val_loss: 0.3268 - val_categorical_accuracy: 0.8975\n",
      "Epoch 237/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1945 - categorical_accuracy: 0.9580Epoch 00236: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1944 - categorical_accuracy: 0.9581 - val_loss: 0.3378 - val_categorical_accuracy: 0.8968\n",
      "Epoch 238/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1856 - categorical_accuracy: 0.9630Epoch 00237: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1854 - categorical_accuracy: 0.9631 - val_loss: 0.3302 - val_categorical_accuracy: 0.8977\n",
      "Epoch 239/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1921 - categorical_accuracy: 0.9596Epoch 00238: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1923 - categorical_accuracy: 0.9596 - val_loss: 0.3287 - val_categorical_accuracy: 0.8970\n",
      "Epoch 240/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1847 - categorical_accuracy: 0.9621Epoch 00239: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1847 - categorical_accuracy: 0.9622 - val_loss: 0.3296 - val_categorical_accuracy: 0.8977\n",
      "Epoch 241/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1831 - categorical_accuracy: 0.9608Epoch 00240: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1830 - categorical_accuracy: 0.9609 - val_loss: 0.3340 - val_categorical_accuracy: 0.8975\n",
      "Epoch 242/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1807 - categorical_accuracy: 0.9626Epoch 00241: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1807 - categorical_accuracy: 0.9627 - val_loss: 0.3572 - val_categorical_accuracy: 0.8982\n",
      "Epoch 243/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1860 - categorical_accuracy: 0.9636Epoch 00242: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1858 - categorical_accuracy: 0.9636 - val_loss: 0.3356 - val_categorical_accuracy: 0.8982\n",
      "Epoch 244/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1853 - categorical_accuracy: 0.9587Epoch 00243: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1851 - categorical_accuracy: 0.9588 - val_loss: 0.3344 - val_categorical_accuracy: 0.8965\n",
      "Epoch 245/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1987 - categorical_accuracy: 0.9571Epoch 00244: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1985 - categorical_accuracy: 0.9572 - val_loss: 0.3437 - val_categorical_accuracy: 0.8977\n",
      "Epoch 246/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1821 - categorical_accuracy: 0.9628Epoch 00245: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1821 - categorical_accuracy: 0.9629 - val_loss: 0.3362 - val_categorical_accuracy: 0.8972\n",
      "Epoch 247/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1822 - categorical_accuracy: 0.9578Epoch 00246: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1819 - categorical_accuracy: 0.9579 - val_loss: 0.3350 - val_categorical_accuracy: 0.8972\n",
      "Epoch 248/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1777 - categorical_accuracy: 0.9609Epoch 00247: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1775 - categorical_accuracy: 0.9610 - val_loss: 0.3325 - val_categorical_accuracy: 0.8970\n",
      "Epoch 249/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1703 - categorical_accuracy: 0.9649Epoch 00248: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1703 - categorical_accuracy: 0.9649 - val_loss: 0.3355 - val_categorical_accuracy: 0.8972\n",
      "Epoch 250/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1704 - categorical_accuracy: 0.9621Epoch 00249: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1704 - categorical_accuracy: 0.9621 - val_loss: 0.3473 - val_categorical_accuracy: 0.8972\n",
      "Epoch 251/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1773 - categorical_accuracy: 0.9607Epoch 00250: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1771 - categorical_accuracy: 0.9607 - val_loss: 0.3442 - val_categorical_accuracy: 0.8963\n",
      "Epoch 252/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1832 - categorical_accuracy: 0.9599Epoch 00251: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1833 - categorical_accuracy: 0.9599 - val_loss: 0.4054 - val_categorical_accuracy: 0.8948\n",
      "Epoch 253/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2490 - categorical_accuracy: 0.9600Epoch 00252: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2488 - categorical_accuracy: 0.9600 - val_loss: 0.3721 - val_categorical_accuracy: 0.8931\n",
      "Epoch 254/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.2046 - categorical_accuracy: 0.9590Epoch 00253: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.2044 - categorical_accuracy: 0.9591 - val_loss: 0.3539 - val_categorical_accuracy: 0.8941\n",
      "Epoch 255/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1886 - categorical_accuracy: 0.9611Epoch 00254: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1884 - categorical_accuracy: 0.9611 - val_loss: 0.3472 - val_categorical_accuracy: 0.8936\n",
      "Epoch 256/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1801 - categorical_accuracy: 0.9638Epoch 00255: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1799 - categorical_accuracy: 0.9639 - val_loss: 0.3291 - val_categorical_accuracy: 0.8946\n",
      "Epoch 257/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1751 - categorical_accuracy: 0.9650Epoch 00256: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1750 - categorical_accuracy: 0.9650 - val_loss: 0.3336 - val_categorical_accuracy: 0.8953\n",
      "Epoch 258/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1740 - categorical_accuracy: 0.9653Epoch 00257: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1739 - categorical_accuracy: 0.9653 - val_loss: 0.3337 - val_categorical_accuracy: 0.8958\n",
      "Epoch 259/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1686 - categorical_accuracy: 0.9678Epoch 00258: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1685 - categorical_accuracy: 0.9679 - val_loss: 0.3206 - val_categorical_accuracy: 0.8963\n",
      "Epoch 260/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1776 - categorical_accuracy: 0.9581Epoch 00259: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1775 - categorical_accuracy: 0.9582 - val_loss: 0.3127 - val_categorical_accuracy: 0.8956\n",
      "Epoch 261/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1678 - categorical_accuracy: 0.9647Epoch 00260: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1678 - categorical_accuracy: 0.9648 - val_loss: 0.3211 - val_categorical_accuracy: 0.8956\n",
      "Epoch 262/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1731 - categorical_accuracy: 0.9609Epoch 00261: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1730 - categorical_accuracy: 0.9609 - val_loss: 0.3195 - val_categorical_accuracy: 0.8960\n",
      "Epoch 263/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1849 - categorical_accuracy: 0.9616Epoch 00262: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1847 - categorical_accuracy: 0.9616 - val_loss: 0.3476 - val_categorical_accuracy: 0.8963\n",
      "Epoch 264/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1781 - categorical_accuracy: 0.9620Epoch 00263: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1780 - categorical_accuracy: 0.9621 - val_loss: 0.3423 - val_categorical_accuracy: 0.8953\n",
      "Epoch 265/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1768 - categorical_accuracy: 0.9588Epoch 00264: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1766 - categorical_accuracy: 0.9589 - val_loss: 0.3327 - val_categorical_accuracy: 0.8963\n",
      "Epoch 266/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1655 - categorical_accuracy: 0.9662Epoch 00265: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1654 - categorical_accuracy: 0.9663 - val_loss: 0.3356 - val_categorical_accuracy: 0.8970\n",
      "Epoch 267/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1696 - categorical_accuracy: 0.9635Epoch 00266: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1695 - categorical_accuracy: 0.9636 - val_loss: 0.3327 - val_categorical_accuracy: 0.8956\n",
      "Epoch 268/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1626 - categorical_accuracy: 0.9644Epoch 00267: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1627 - categorical_accuracy: 0.9645 - val_loss: 0.3280 - val_categorical_accuracy: 0.8958\n",
      "Epoch 269/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1709 - categorical_accuracy: 0.9638Epoch 00268: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1706 - categorical_accuracy: 0.9639 - val_loss: 0.3214 - val_categorical_accuracy: 0.8956\n",
      "Epoch 270/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1671 - categorical_accuracy: 0.9648Epoch 00269: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1669 - categorical_accuracy: 0.9649 - val_loss: 0.3298 - val_categorical_accuracy: 0.8972\n",
      "Epoch 271/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1574 - categorical_accuracy: 0.9667Epoch 00270: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1572 - categorical_accuracy: 0.9668 - val_loss: 0.3286 - val_categorical_accuracy: 0.8977\n",
      "Epoch 272/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1629 - categorical_accuracy: 0.9656Epoch 00271: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1629 - categorical_accuracy: 0.9656 - val_loss: 0.3333 - val_categorical_accuracy: 0.8975\n",
      "Epoch 273/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1602 - categorical_accuracy: 0.9625Epoch 00272: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1600 - categorical_accuracy: 0.9626 - val_loss: 0.3371 - val_categorical_accuracy: 0.8977\n",
      "Epoch 274/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1619 - categorical_accuracy: 0.9639Epoch 00273: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1617 - categorical_accuracy: 0.9640 - val_loss: 0.3287 - val_categorical_accuracy: 0.8980\n",
      "Epoch 275/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1615 - categorical_accuracy: 0.9623Epoch 00274: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1614 - categorical_accuracy: 0.9624 - val_loss: 0.3250 - val_categorical_accuracy: 0.8975\n",
      "Epoch 276/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1623 - categorical_accuracy: 0.9635Epoch 00275: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1622 - categorical_accuracy: 0.9636 - val_loss: 0.3277 - val_categorical_accuracy: 0.8977\n",
      "Epoch 277/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1588 - categorical_accuracy: 0.9681Epoch 00276: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1587 - categorical_accuracy: 0.9681 - val_loss: 0.3333 - val_categorical_accuracy: 0.8977\n",
      "Epoch 278/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1599 - categorical_accuracy: 0.9656Epoch 00277: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1597 - categorical_accuracy: 0.9657 - val_loss: 0.3290 - val_categorical_accuracy: 0.8965\n",
      "Epoch 279/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1641 - categorical_accuracy: 0.9599Epoch 00278: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1639 - categorical_accuracy: 0.9600 - val_loss: 0.3343 - val_categorical_accuracy: 0.8975\n",
      "Epoch 280/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1542 - categorical_accuracy: 0.9691Epoch 00279: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1541 - categorical_accuracy: 0.9691 - val_loss: 0.3419 - val_categorical_accuracy: 0.8980\n",
      "Epoch 281/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1514 - categorical_accuracy: 0.9671Epoch 00280: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1512 - categorical_accuracy: 0.9671 - val_loss: 0.3494 - val_categorical_accuracy: 0.8980\n",
      "Epoch 282/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1755 - categorical_accuracy: 0.9657Epoch 00281: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1756 - categorical_accuracy: 0.9657 - val_loss: 0.3596 - val_categorical_accuracy: 0.8972\n",
      "Epoch 283/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1608 - categorical_accuracy: 0.9658Epoch 00282: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1606 - categorical_accuracy: 0.9659 - val_loss: 0.3541 - val_categorical_accuracy: 0.8985\n",
      "Epoch 284/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1587 - categorical_accuracy: 0.9646Epoch 00283: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1585 - categorical_accuracy: 0.9647 - val_loss: 0.3439 - val_categorical_accuracy: 0.8980\n",
      "Epoch 285/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1573 - categorical_accuracy: 0.9648Epoch 00284: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1572 - categorical_accuracy: 0.9648 - val_loss: 0.3526 - val_categorical_accuracy: 0.8977\n",
      "Epoch 286/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1539 - categorical_accuracy: 0.9673Epoch 00285: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1537 - categorical_accuracy: 0.9673 - val_loss: 0.3456 - val_categorical_accuracy: 0.8980\n",
      "Epoch 287/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1490 - categorical_accuracy: 0.9702Epoch 00286: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1489 - categorical_accuracy: 0.9703 - val_loss: 0.3341 - val_categorical_accuracy: 0.8977\n",
      "Epoch 288/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1556 - categorical_accuracy: 0.9680Epoch 00287: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1555 - categorical_accuracy: 0.9680 - val_loss: 0.3403 - val_categorical_accuracy: 0.8972\n",
      "Epoch 289/1000\n",
      "37120/37220 [============================>.] - ETA: 0s - loss: 0.1589 - categorical_accuracy: 0.9643Epoch 00288: val_loss did not improve\n",
      "37220/37220 [==============================] - 41s - loss: 0.1587 - categorical_accuracy: 0.9643 - val_loss: 0.3544 - val_categorical_accuracy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "history = model.fit(\n",
    "    x = dataset_tot,\n",
    "    y = encoded,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 1000,\n",
    "    validation_split=.10,\n",
    "    #validation_steps=10,\n",
    "    shuffle=False,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=200),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=45, factor=0.5, min_lr=1e-5),\n",
    "        mc\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/npasini1/Desktop/model_checkpoints/modelsave.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4064/4136 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.058333030173183927, 0.9929883945841392]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('/home/npasini1/nicolo_ws/src/pyrecorder/src/bestmodel_v2_senzapreprocessing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(X_test)\n",
    "print(X_test.shape)\n",
    "prediction = np.argmax(a, axis=1)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 2.7.17 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
