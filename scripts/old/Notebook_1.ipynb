{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow==2.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 10:20:26.312229: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-28 10:20:26.312249: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(tf.__version__)\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "from scipy.io import loadmat\n",
    "\n",
    "camera_delta_labelled_mat = loadmat(\"/home/nicolo/Scrivania/mat/camera_delta_labelled_3.mat\")\n",
    "windows_labelled_mat = loadmat(\"/home/nicolo/Scrivania/mat/windows_labelled_3.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47219, 4)\n",
      "(47219, 31, 7)\n"
     ]
    }
   ],
   "source": [
    "# Conversion to numpy array\n",
    "camera_delta_labelled = np.transpose(camera_delta_labelled_mat['camera_delta_labelled'].astype('float64'))\n",
    "windows_labelled = np.transpose(windows_labelled_mat['windows_labelled'].astype('float64'),(1,2,0))\n",
    "\n",
    "\n",
    "print(camera_delta_labelled.shape)\n",
    "print(windows_labelled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices reshaping --> I need to reshape in order to have the order (windows , length , features).\n",
    "# I cannot use reshape since it will change the order of the windows inside the matrix randomly,\n",
    "# but at the same time using only rot90 will not be enough, i also need to flip since the rot90\n",
    "# will invert last and first elements (last become first and viceversa)\n",
    "\n",
    "# print(camera_delta_labelled[-1,29])\n",
    "# print(windows_labelled[:,29,-1])\n",
    "\n",
    "# a1 = np.rot90(camera_delta_labelled,1)\n",
    "# a2 = np.flip(a1,0)\n",
    "# camera_delta_labelled_rot = a2\n",
    "\n",
    "# b1=np.rot90(windows_labelled, -1, axes=(0,1))\n",
    "# b2=np.flip(b1,1)\n",
    "# b3=np.rot90(b2, 1, axes=(1,2))\n",
    "# b4=np.flip(b3,1)\n",
    "# windows_labelled_rot = b4\n",
    "\n",
    "# print(camera_delta_labelled_rot.shape)\n",
    "# print(windows_labelled_rot.shape)\n",
    "\n",
    "# # Check: 29th and 120th elements are belonging to class 1, let's see if it still true\n",
    "# print(camera_delta_labelled_rot[29,-1]) #or 120\n",
    "# print(windows_labelled_rot[29,-1,:])    #or 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements belonging to class 1:  112\n",
      "Number of elements belonging to class 0:  47107\n",
      "Number of elements belonging to class 1:  112\n",
      "Number of elements belonging to class 0:  47107\n",
      "Number of elements belonging to class 1:  0\n",
      "Number of elements belonging to class 0:  47219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the labels matrix\n",
    "# Check whether the matlab file creation has been done correctly (labels extracted from both the matrices should be the same,\n",
    "# and in the same position)\n",
    "\n",
    "labels = camera_delta_labelled[:,-1]\n",
    "print('Number of elements belonging to class 1: ', np.count_nonzero(labels == 1))\n",
    "print('Number of elements belonging to class 0: ', np.count_nonzero(labels == 0))\n",
    "\n",
    "labels_check = windows_labelled[:,-1,0]\n",
    "print('Number of elements belonging to class 1: ', np.count_nonzero(labels_check == 1))\n",
    "print('Number of elements belonging to class 0: ', np.count_nonzero(labels_check == 0))\n",
    "\n",
    "check = labels - labels_check\n",
    "print('Number of elements belonging to class 1: ', np.count_nonzero(check == 1))\n",
    "print('Number of elements belonging to class 0: ', np.count_nonzero(check == 0))\n",
    "np.sum(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix containing camera windows labels at each timestamp:  (47219, 1)\n",
      "Matrix containing kinematic features at each timestamp:  (47219, 30, 7)\n"
     ]
    }
   ],
   "source": [
    "# Matrices creation (predictions and features)\n",
    "# Matrix containing the delta movements (dx,dy,dz) or the class (whether we are doing classification or regression --> TO BE DONE simultaneously in the next step)\n",
    "\n",
    "# In this case: CLASSIFICATION\n",
    "delta_cam = camera_delta_labelled[:,-1:]\n",
    "print('Matrix containing camera windows labels at each timestamp: ', delta_cam.shape)\n",
    "\n",
    "# In this case: REGRESSION\n",
    "# delta_cam = camera_delta_labelled[:,:-1]\n",
    "# print('Matrix containing camera delta movements at each timestamp: ', delta_cam.shape)\n",
    "\n",
    "kinematic_windows = windows_labelled[:,:-1,:]\n",
    "print('Matrix containing kinematic features at each timestamp: ', kinematic_windows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (30, 7)\n",
      "Output shape:  (1,)\n"
     ]
    }
   ],
   "source": [
    "input_shape = kinematic_windows.shape[1:]\n",
    "output_shape = delta_cam.shape[1:]\n",
    "print('Input shape: ', input_shape)\n",
    "print('Output shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 30, 7)\n",
      "(35000, 1)\n",
      "(12219, 30, 7)\n",
      "(12219, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = kinematic_windows[:35000,:,:]\n",
    "y_train = delta_cam[:35000,:]\n",
    "X_val = kinematic_windows[35000:,:,:]\n",
    "y_val = delta_cam[35000:,:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from random import randrange\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Concatenate\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input, BatchNormalization, \\\n",
    "    multiply, concatenate, Flatten, Activation, dot\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from IPython.display import FileLink, FileLinks\n",
    "import warnings\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cnn_lstm_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None, 30, 7)]           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 30, 128)           36864     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 30, 64)            24640     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 15, 128)           66048     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 15, 128)           49280     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 176,961\n",
      "Trainable params: 176,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "def build_CONV_LSTM_model(input_shape, output_shape):\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    convlstm = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True))(input_layer)\n",
    "    convlstm = tfkl.Conv1D(64, 3, padding='same', activation='relu')(convlstm)\n",
    "    convlstm = tfkl.MaxPool1D()(convlstm)\n",
    "    convlstm = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True))(convlstm)\n",
    "    convlstm = tfkl.Conv1D(128, 3, padding='same', activation='relu')(convlstm)\n",
    "    convlstm = tfkl.GlobalAveragePooling1D()(convlstm)\n",
    "    convlstm = tfkl.Dropout(.3)(convlstm)\n",
    "\n",
    "    # In order to predict the next values for more than one sensor,\n",
    "    # we can use a Dense layer with a number given by telescope*num_sensors,\n",
    "    # followed by a Reshape layer to obtain a tensor of dimension \n",
    "    # [None, telescope, num_sensors]\n",
    "    output_layer = tfkl.Dense(output_shape[-1]*1, activation='sigmoid')(convlstm)\n",
    "    #output_layer = tfkl.Reshape((1,output_shape[-1]))(output_layer)\n",
    "    #output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same')(output_layer)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='cnn_lstm_model')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(lr = 1e-3),\n",
    "                  metrics=tf.keras.metrics.Accuracy())\n",
    "    # Return the model\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_CONV_LSTM_model(input_shape, output_shape)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 1.0, 1.0: 4.146932776767107}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# labels_dict : {ind_label: count_label}\n",
    "# mu : parameter to tune \n",
    "\n",
    "def create_class_weight(labels_dict,mu=0.15):\n",
    "    total = np.sum(list(labels_dict.values()))\n",
    "    keys = labels_dict.keys()\n",
    "    class_weights = dict()\n",
    "    \n",
    "    for key in keys:\n",
    "        score = math.log(mu*total/float(labels_dict[key]))\n",
    "        class_weights[key] = score if score > 1.0 else 1.0\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "labels_dict = {0.0: 47107,\n",
    "               1.0: 112}\n",
    "\n",
    "class_weights = create_class_weight(labels_dict)\n",
    "pprint.pprint(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "67/67 [==============================] - 14s 149ms/step - loss: 0.0941 - accuracy: 0.0000e+00 - val_loss: 0.0238 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 9s 137ms/step - loss: 0.0596 - accuracy: 0.0000e+00 - val_loss: 0.0181 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 9s 136ms/step - loss: 0.0591 - accuracy: 0.0000e+00 - val_loss: 0.0154 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 9s 136ms/step - loss: 0.0605 - accuracy: 0.0000e+00 - val_loss: 0.0162 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 9s 139ms/step - loss: 0.0601 - accuracy: 0.0000e+00 - val_loss: 0.0231 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 9s 138ms/step - loss: 0.0601 - accuracy: 0.0000e+00 - val_loss: 0.0213 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 9s 138ms/step - loss: 0.0590 - accuracy: 0.0000e+00 - val_loss: 0.0181 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 9s 137ms/step - loss: 0.0585 - accuracy: 0.0000e+00 - val_loss: 0.0187 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 9s 129ms/step - loss: 0.0589 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 9s 129ms/step - loss: 0.0583 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 10s 146ms/step - loss: 0.0582 - accuracy: 0.0000e+00 - val_loss: 0.0192 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 9s 136ms/step - loss: 0.0586 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 8s 123ms/step - loss: 0.0578 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 9s 129ms/step - loss: 0.0580 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 8s 125ms/step - loss: 0.0593 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 8s 123ms/step - loss: 0.0582 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 0.0586 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 9s 132ms/step - loss: 0.0587 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 9s 130ms/step - loss: 0.0591 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 8s 120ms/step - loss: 0.0588 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 8s 118ms/step - loss: 0.0581 - accuracy: 0.0000e+00 - val_loss: 0.0191 - val_accuracy: 0.0000e+00\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 526\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = (X_val,  y_val),\n",
    "    class_weight = class_weights,\n",
    "    callbacks = [\n",
    "        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='min', patience=20, \n",
    "                                    restore_best_weights=True, verbose = 1),\n",
    "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=3,\n",
    "                                        factor=0.1, min_lr=1e-7, verbose = 1)\n",
    "    ]\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penso che i problemi siano molteplici:\n",
    "# - le classi sono parecchio imbalanced, quindi fa veramente fatica ad imparare \n",
    "# - non capisco se ho scritto correttamente il modello: alla seconda epoca sembra avere un miglioramento, come se ne azzeccasse una su 112 invece che 0, o simile\n",
    "# - nonstante il miglioramento in training, sul validation fa comunque pena (non son riuscito a mettere balanced accuracy, e accuracy non è corretta come metrica)\n",
    "# - vorrei capire se ha senso rifare un dataset più corposo per la classe 1, diciamo un dataset in cui ripeto sempre le stesse mosse per vedere se riesce ad allenare quando\n",
    "#   vede movimenti ripetitivi, perchè questo dataset che ho usato viene da esperimenti lunghi in cui i movimenti, e quindi le finestre dei dati di cinematica sono, sono\n",
    "#   molto diversi fra loro\n",
    "\n",
    "#focal loss crossentropy\n",
    "#tresholding\n",
    "#np.transpose\n",
    "#invece che acuracy, report completo con micro,macro confusion matrix"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
